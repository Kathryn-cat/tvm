# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(128, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32])
                            T.writes(C[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 256])
                            C_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32])
                                T.writes(C_reindex_shared[0 : 32, 0 : 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(2):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, ax2_0_0 * 256 : ax2_0_0 * 256 + 256], B[ax2_0_0 * 256 : ax2_0_0 * 256 + 256, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        A_reindex_shared = T.alloc_buffer([32, 256], dtype="float16", strides=[264, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([32, 256], dtype="float16", strides=[264, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0_ax1_fused_0 // 2, ax2_0_0 * 256 + (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 // 2, (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 // 2, (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256] = A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0_ax1_fused_0 // 2, ax2_0_0 * 256 + (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256]
                                        for ax0_ax1_fused_0 in T.serial(256):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 256 + ax0_ax1_fused_0 % 8 * 32 + threadIdx_x, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + ax0_ax1_fused_0 // 8])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 // 8, ax0_ax1_fused_0 % 8 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 // 8, ax0_ax1_fused_0 % 8 * 32 + threadIdx_x] = B[ax2_0_0 * 256 + ax0_ax1_fused_0 % 8 * 32 + threadIdx_x, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + ax0_ax1_fused_0 // 8]
                                        for ax2_0_1 in T.serial(8):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 32, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], B_reindex_shared[0 : 32, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 4224 + ax2_0_1 * 32 + ax1_0 * 16, 4224, 1, dtype="handle"), 264, "row_major", dtype="handle"))
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 4224 + ax2_0_1 * 32 + ax1_0 * 16, 4224, 1, dtype="handle"), 264, "col_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3, ax2_0_2 in T.grid(2, 2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax1_0_3 * 16 : ax1_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 2 + ax2_0_2, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + ax1_0 * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 8 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3]
    

