2022-11-27 18:52:11 [INFO] Logging directory: logs/logs
2022-11-27 18:52:11 [INFO] LocalBuilder: max_workers = 32
2022-11-27 18:52:11 [INFO] LocalRunner: max_workers = 1
2022-11-27 18:52:12 [INFO] [task_scheduler.cc:157] Initializing Task #0: "main"
2022-11-27 18:52:12 [INFO] [task_scheduler.cc:178] TaskScheduler picks Task #0: "main"
2022-11-27 18:52:53 [INFO] [task_scheduler.cc:191] Sending 32 sample(s) to builder
2022-11-27 18:53:00 [INFO] [task_scheduler.cc:193] Sending 32 sample(s) to runner
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(16, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 128 : blockIdx_y * 128 + 128, 0 : 512], B[0 : 512, blockIdx_x * 64 : blockIdx_x * 64 + 64])
                            T.writes(C[blockIdx_y * 128 : blockIdx_y * 128 + 128, blockIdx_x * 64 : blockIdx_x * 64 + 64])
                            C_reindex_shared = T.alloc_buffer([128, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 128 : blockIdx_y * 128 + 128, 0 : 512], B[0 : 512, blockIdx_x * 64 : blockIdx_x * 64 + 64])
                                T.writes(C_reindex_shared[threadIdx_y * 64 : threadIdx_y * 64 + 64, 0 : 64])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([64, 64], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init, ax0_0_4_init in T.grid(4, 4):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init * 4 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(32):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 128 : blockIdx_y * 128 + 128, ax2_0_0 * 16 : ax2_0_0 * 16 + 16], B[ax2_0_0 * 16 : ax2_0_0 * 16 + 16, blockIdx_x * 64 : blockIdx_x * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                        B_reindex_shared = T.alloc_buffer([64, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([128, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([64, 16], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([64, 16], dtype="float16", scope="wmma.matrix_a")
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y * 128 + ax0_ax1_fused_0 * 32 + threadIdx_y * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 16, ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 32 + threadIdx_y * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 16, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 32 + threadIdx_y * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 16, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16] = A[blockIdx_y * 128 + ax0_ax1_fused_0 * 32 + threadIdx_y * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 16, ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16]
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 16 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 16, blockIdx_x * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 16] = B[ax2_0_0 * 16 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 16, blockIdx_x * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16]
                                        for ax0_0 in T.serial(4):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[threadIdx_y * 64 + ax0_0 * 16 : threadIdx_y * 64 + ax0_0 * 16 + 16, 0 : 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 2560 + ax0_0 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax0_0 in T.serial(4):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 640, 640, 1, dtype="handle"), 40, "col_major", dtype="handle"))
                                        for ax1_0_3, ax0_0_4 in T.grid(4, 4):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[ax1_0_3 * 16 : ax1_0_3 * 16 + 16, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 4 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_4, B_reindex_shared_wmma_matrix_b.data, ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 4 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(4, 4):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 64 + ax0_0 * 16 : threadIdx_y * 64 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 4096 + ax0_0 * 1024 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(128):
                                with T.block("C_reindex_shared"):
                                    T.reads(C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x])
                                    T.writes(C[blockIdx_y * 128 + ax0, blockIdx_x * 64 + threadIdx_y * 32 + threadIdx_x])
                                    C[blockIdx_y * 128 + ax0, blockIdx_x * 64 + threadIdx_y * 32 + threadIdx_x] = C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(32, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(64, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 64 : blockIdx_y * 64 + 64, 0 : 512], B[0 : 512, blockIdx_x * 32 : blockIdx_x * 32 + 32])
                            T.writes(C[blockIdx_y * 64 : blockIdx_y * 64 + 64, blockIdx_x * 32 : blockIdx_x * 32 + 512])
                            C_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 64 : blockIdx_y * 64 + 64, 0 : 512], B[0 : 512, blockIdx_x * 32 : blockIdx_x * 32 + 32])
                                T.writes(C_reindex_shared[0 : 64, threadIdx_y * 16 : threadIdx_y * 16 + 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([64, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax0_0_4_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 32 + ax0_0_4_init * 16 : ax0_0_3_init * 32 + ax0_0_4_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax0_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 64 : blockIdx_y * 64 + 64, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_x * 32 : blockIdx_x * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 64, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 64, 0 : 16])
                                        A_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32] = A[blockIdx_y * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_y, blockIdx_x * 32 + threadIdx_x])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x] = B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_y, blockIdx_x * 32 + threadIdx_x]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 64, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y * 16 : threadIdx_y * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 64, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 64, 0 : 16])
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([64, 16], dtype="float16", scope="wmma.matrix_a")
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_b")
                                                for ax0_0 in T.serial(4):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                    T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y * 16 : threadIdx_y * 16 + 16])
                                                    T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                    T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 640 + threadIdx_y * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax0_0_3, ax0_0_4 in T.grid(2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax0_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax0_0_4, B_reindex_shared_wmma_matrix_b.data, 0, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax0_0_4, dtype="handle"))
                                for ax0_0 in T.serial(4):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, threadIdx_y * 16 : threadIdx_y * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + threadIdx_y * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 256 + threadIdx_x * 8 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y * 64 + ax0, threadIdx_y * 256 + blockIdx_x * 32 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y * 64 + ax0, threadIdx_y * 256 + blockIdx_x * 32 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(2048, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(4, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 128 * 128 + blockIdx_x * 32 : blockIdx_y // 128 * 128 + blockIdx_x * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 128 * 16 : blockIdx_y % 128 * 16 + 16])
                            T.writes(C[blockIdx_y // 128 * 128 + blockIdx_x * 32 : blockIdx_y // 128 * 128 + blockIdx_x * 32 + 32, blockIdx_y % 128 * 16 : blockIdx_y % 128 * 16 + 128])
                            C_reindex_shared = T.alloc_buffer([32, 16], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 128 * 128 + blockIdx_x * 32 : blockIdx_y // 128 * 128 + blockIdx_x * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 128 * 16 : blockIdx_y % 128 * 16 + 16])
                                T.writes(C_reindex_shared[0 : 32, 0 : 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_4_init in T.serial(2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(4):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 128 * 128 + blockIdx_x * 32 : blockIdx_y // 128 * 128 + blockIdx_x * 32 + 32, ax2_0_0 * 128 : ax2_0_0 * 128 + 128], B[ax2_0_0 * 128 : ax2_0_0 * 128 + 128, blockIdx_y % 128 * 16 : blockIdx_y % 128 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        A_reindex_shared = T.alloc_buffer([32, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([16, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y // 128 * 128 + blockIdx_x * 32 + ax0_ax1_fused_0 // 4, ax2_0_0 * 128 + ax0_ax1_fused_0 % 4 * 32 + threadIdx_x])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 // 4, ax0_ax1_fused_0 % 4 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 // 4, ax0_ax1_fused_0 % 4 * 32 + threadIdx_x] = A[blockIdx_y // 128 * 128 + blockIdx_x * 32 + ax0_ax1_fused_0 // 4, ax2_0_0 * 128 + ax0_ax1_fused_0 % 4 * 32 + threadIdx_x]
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 128 + ax0_ax1_fused_0 % 4 * 32 + threadIdx_x, blockIdx_y % 128 * 16 + ax0_ax1_fused_0 // 4])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 // 4, ax0_ax1_fused_0 % 4 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 // 4, ax0_ax1_fused_0 % 4 * 32 + threadIdx_x] = B[ax2_0_0 * 128 + ax0_ax1_fused_0 % 4 * 32 + threadIdx_x, blockIdx_y % 128 * 16 + ax0_ax1_fused_0 // 4]
                                        for ax2_0_1 in T.serial(8):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 32, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[0 : 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_a")
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_b")
                                                for ax0_0 in T.serial(2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 2176 + ax2_0_1 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                                with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                    T.reads(B_reindex_shared[0 : 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                    T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                    T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 16, 2176, 1, dtype="handle"), 136, "col_major", dtype="handle"))
                                                for ax0_0_4 in T.serial(2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_4, B_reindex_shared_wmma_matrix_b.data, 0, C_reindex_shared_wmma_accumulator.data, ax0_0_4, dtype="handle"))
                                for ax0_0 in T.serial(2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 256, 256, 2, dtype="handle"), 16, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 4 + ax1_3 < 16)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y // 128 * 128 + blockIdx_x * 32 + ax0, blockIdx_y % 128 * 16 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y // 128 * 128 + blockIdx_x * 32 + ax0, blockIdx_y % 128 * 16 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(2, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(4096, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 1024 + blockIdx_x // 128 * 32 : blockIdx_y * 1024 + blockIdx_x // 128 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 128 * 16 : blockIdx_x % 128 * 16 + 16])
                            T.writes(C[blockIdx_y * 1024 + blockIdx_x // 128 * 32 : blockIdx_y * 1024 + blockIdx_x // 128 * 32 + 32, blockIdx_x % 128 * 16 : blockIdx_x % 128 * 16 + 256])
                            C_reindex_shared = T.alloc_buffer([32, 16], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 1024 + blockIdx_x // 128 * 32 : blockIdx_y * 1024 + blockIdx_x // 128 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 128 * 16 : blockIdx_x % 128 * 16 + 16])
                                T.writes(C_reindex_shared[0 : 32, 0 : 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_4_init in T.serial(2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 1024 + blockIdx_x // 128 * 32 : blockIdx_y * 1024 + blockIdx_x // 128 * 32 + 32, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_x % 128 * 16 : blockIdx_x % 128 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        A_reindex_shared = T.alloc_buffer([32, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([64, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y * 1024 + blockIdx_x // 128 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_x // 16, ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_x // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_x // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64] = A[blockIdx_y * 1024 + blockIdx_x // 128 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_x // 16, ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_x // 8, blockIdx_x % 128 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_x // 8, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_x // 8, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16] = B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_x // 8, blockIdx_x % 128 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 32, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], B_reindex_shared[ax2_0_1 * 32 : ax2_0_1 * 32 + 32, 0 : 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 1152 + ax2_0_1 * 32 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                                for ax0_0 in T.serial(2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 32 + ax0_0 * 16 : ax2_0_1 * 32 + ax0_0 * 16 + 16, 0 : 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 1280 + ax0_0 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax2_0_2, ax0_0_4 in T.grid(2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, 0 : 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_4 * 2 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax2_0_2, C_reindex_shared_wmma_accumulator.data, ax0_0_4, dtype="handle"))
                                for ax0_0 in T.serial(2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 256, 256, 2, dtype="handle"), 16, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 8 + ax1_3 < 16)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y * 1024 + blockIdx_x // 128 * 32 + ax0, blockIdx_x % 128 * 16 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y * 1024 + blockIdx_x // 128 * 32 + ax0, blockIdx_x % 128 * 16 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(4, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(64, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(8, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_x // 4 * 128 : blockIdx_x // 4 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_y * 512 + blockIdx_x % 4 * 128 : blockIdx_y * 512 + blockIdx_x % 4 * 128 + 128])
                            T.writes(C[blockIdx_x // 4 * 128 : blockIdx_x // 4 * 128 + 128, blockIdx_y * 512 + blockIdx_x % 4 * 128 : blockIdx_y * 512 + blockIdx_x % 4 * 128 + 2048])
                            C_reindex_shared = T.alloc_buffer([128, 128], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_x // 4 * 128 : blockIdx_x // 4 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_y * 512 + blockIdx_x % 4 * 128 : blockIdx_y * 512 + blockIdx_x % 4 * 128 + 128])
                                T.writes(C_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, 0 : 128])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([16, 128], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init, ax1_0_4_init in T.grid(2, 4):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3_init * 64 + ax1_0_4_init * 16 : ax1_0_3_init * 64 + ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0_3_init * 4 + ax1_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(32):
                                    with T.block():
                                        T.reads(A[blockIdx_x // 4 * 128 : blockIdx_x // 4 * 128 + 128, ax2_0_0 * 16 : ax2_0_0 * 16 + 16], B[ax2_0_0 * 16 : ax2_0_0 * 16 + 16, blockIdx_y * 512 + blockIdx_x % 4 * 128 : blockIdx_y * 512 + blockIdx_x % 4 * 128 + 128], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 128])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 128])
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 128], dtype="float16", scope="wmma.matrix_b")
                                        B_reindex_shared = T.alloc_buffer([16, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_a")
                                        A_reindex_shared = T.alloc_buffer([128, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_x // 4 * 128 + ax0_ax1_fused_0 * 32 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, ax2_0_0 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 32 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 32 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16] = A[blockIdx_x // 4 * 128 + ax0_ax1_fused_0 * 32 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, ax2_0_0 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16]
                                        for ax0_ax1_fused_0 in T.serial(2):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 8 + threadIdx_y, blockIdx_y * 512 + blockIdx_x % 4 * 128 + (threadIdx_x * 4 + ax0_ax1_fused_3)])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y, threadIdx_x * 4 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y, threadIdx_x * 4 + ax0_ax1_fused_3] = B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 8 + threadIdx_y, blockIdx_y * 512 + blockIdx_x % 4 * 128 + (threadIdx_x * 4 + ax0_ax1_fused_3)]
                                        with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                            T.reads(A_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, 0 : 16])
                                            T.writes(A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16])
                                            T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax1_0 in T.serial(8):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax1_0 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                        for ax1_0_3, ax1_0_4 in T.grid(2, 4):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 64 + ax1_0_4 * 16 : ax1_0_3 * 64 + ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 64 + ax1_0_4 * 16 : ax1_0_3 * 64 + ax1_0_4 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 64 + ax1_0_4 * 16 : ax1_0_3 * 64 + ax1_0_4 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax1_0_3 * 4 + ax1_0_4, A_reindex_shared_wmma_matrix_a.data, 0, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 4 + ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax1_0_3 * 4 + ax1_0_4, dtype="handle"))
                                for ax1_0 in T.serial(8):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 2048 + ax1_0 * 16, 2048, 2, dtype="handle"), 128, "row_major", dtype="handle"))
                            for ax0 in T.serial(128):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 256 + threadIdx_x * 8 + ax1_3 < 128)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_x // 4 * 128 + ax0, blockIdx_y * 512 + threadIdx_y * 256 + blockIdx_x % 4 * 128 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_x // 4 * 128 + ax0, blockIdx_y * 512 + threadIdx_y * 256 + blockIdx_x % 4 * 128 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(256, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(16, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 : blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + 16])
                            T.writes(C[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + 64, blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 : blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + 512])
                            C_reindex_shared = T.alloc_buffer([64, 16], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 : blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + 16])
                                T.writes(C_reindex_shared[threadIdx_y * 32 : threadIdx_y * 32 + 32, 0 : 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_4_init in T.serial(2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + 64, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 : blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        B_reindex_shared = T.alloc_buffer([16, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 32])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, (threadIdx_x * 8 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, (threadIdx_x * 8 + ax0_ax1_fused_3) % 32] = A[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 32]
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 32 + threadIdx_x, blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + ax0_ax1_fused_0 * 2 + threadIdx_y])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x] = B[ax2_0_0 * 32 + threadIdx_x, blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + ax0_ax1_fused_0 * 2 + threadIdx_y]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[threadIdx_y * 32 : threadIdx_y * 32 + 32, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[0 : 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0 in T.serial(2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 1280 + ax0_0 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                    T.reads(B_reindex_shared[0 : 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                    T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                    T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "col_major", dtype="handle"))
                                                for ax0_0_4 in T.serial(2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_4, B_reindex_shared_wmma_matrix_b.data, 0, C_reindex_shared_wmma_accumulator.data, ax0_0_4, dtype="handle"))
                                for ax0_0 in T.serial(2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, 0 : 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 512 + ax0_0 * 256, 256, 2, dtype="handle"), 16, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 256 + threadIdx_x * 8 + ax1_3 < 16)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + ax0, threadIdx_y * 256 + blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y // 16 * 128 + blockIdx_x // 8 * 64 + ax0, threadIdx_y * 256 + blockIdx_y % 16 * 128 + blockIdx_x % 8 * 16 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(2, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(128, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(4, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 1024 + blockIdx_x // 4 * 32 : blockIdx_y * 1024 + blockIdx_x // 4 * 32 + 64, 0 : 512], B[0 : 512, blockIdx_x % 4 * 512 : blockIdx_x % 4 * 512 + 512])
                            T.writes(C[blockIdx_y * 1024 + blockIdx_x // 4 * 32 : blockIdx_y * 1024 + blockIdx_x // 4 * 32 + 32, blockIdx_x % 4 * 512 : blockIdx_x % 4 * 512 + 1024])
                            C_reindex_shared = T.alloc_buffer([32, 512], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 1024 + blockIdx_x // 4 * 32 : blockIdx_y * 1024 + blockIdx_x // 4 * 32 + 64, 0 : 512], B[0 : 512, blockIdx_x % 4 * 512 : blockIdx_x % 4 * 512 + 512])
                                T.writes(C_reindex_shared[threadIdx_y // 2 * 16 : threadIdx_y // 2 * 16 + 16, threadIdx_y % 2 * 256 : threadIdx_y % 2 * 256 + 256])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([16, 256], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init, ax1_0_4_init in T.grid(8, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3_init * 32 + ax1_0_4_init * 16 : ax1_0_3_init * 32 + ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0_3_init * 2 + ax1_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(32):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 1024 + blockIdx_x // 4 * 32 : blockIdx_y * 1024 + blockIdx_x // 4 * 32 + 64, ax2_0_0 * 16 : ax2_0_0 * 16 + 16], B[ax2_0_0 * 16 : ax2_0_0 * 16 + 16, blockIdx_x % 4 * 512 : blockIdx_x % 4 * 512 + 512], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 256])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 256])
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_a")
                                        A_reindex_shared = T.alloc_buffer([32, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 256], dtype="float16", scope="wmma.matrix_b")
                                        B_reindex_shared = T.alloc_buffer([16, 512], dtype="float16", strides=[520, 1], scope="shared")
                                        for ax0_ax1_fused_3 in T.vectorized(8):
                                            with T.block("A_reindex_shared"):
                                                T.where(threadIdx_y * 256 + threadIdx_x * 8 + ax0_ax1_fused_3 < 512)
                                                T.reads(A[blockIdx_y * 1024 + blockIdx_x // 4 * 32 + threadIdx_y * 16 + threadIdx_x // 2, ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16])
                                                T.writes(A_reindex_shared[threadIdx_y * 16 + threadIdx_x // 2, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[threadIdx_y * 16 + threadIdx_x // 2, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16] = A[blockIdx_y * 1024 + blockIdx_x // 4 * 32 + threadIdx_y * 16 + threadIdx_x // 2, ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 16 + ax0_ax1_fused_0, blockIdx_x % 4 * 512 + (threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3)])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0, threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0, threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3] = B[ax2_0_0 * 16 + ax0_ax1_fused_0, blockIdx_x % 4 * 512 + (threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3)]
                                        with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                            T.reads(A_reindex_shared[threadIdx_y // 2 * 16 : threadIdx_y // 2 * 16 + 16, 0 : 16])
                                            T.writes(A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16])
                                            T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y // 2 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax1_0 in T.serial(16):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[0 : 16, threadIdx_y % 2 * 256 + ax1_0 * 16 : threadIdx_y % 2 * 256 + ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, threadIdx_y % 2 * 256 + ax1_0 * 16, 8320, 1, dtype="handle"), 520, "row_major", dtype="handle"))
                                        for ax1_0_3, ax1_0_4 in T.grid(8, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax1_0_3 * 2 + ax1_0_4, A_reindex_shared_wmma_matrix_a.data, 0, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 2 + ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax1_0_3 * 2 + ax1_0_4, dtype="handle"))
                                for ax1_0 in T.serial(16):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y // 2 * 16 : threadIdx_y // 2 * 16 + 16, threadIdx_y % 2 * 256 + ax1_0 * 16 : threadIdx_y % 2 * 256 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y // 2 * 8192 + threadIdx_y % 2 * 256 + ax1_0 * 16, 8192, 2, dtype="handle"), 512, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 256 + threadIdx_x * 8 + ax1_3 < 512)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y * 1024 + blockIdx_x // 4 * 32 + ax0, blockIdx_x % 4 * 512 + threadIdx_y * 256 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y * 1024 + blockIdx_x // 4 * 32 + ax0, blockIdx_x % 4 * 512 + threadIdx_y * 256 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(128, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(2, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512])
                            T.writes(C[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512])
                            C_reindex_shared = T.alloc_buffer([32, 512], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512])
                                T.writes(C_reindex_shared[0 : 32, threadIdx_y * 256 : threadIdx_y * 256 + 256])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 256], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init, ax1_0_4_init in T.grid(2, 8, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 32 + ax1_0_4_init * 16 : ax1_0_3_init * 32 + ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 16 + (ax1_0_3_init * 32 + ax1_0_4_init * 16) // 16, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 256])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 256])
                                        A_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([32, 512], dtype="float16", strides=[520, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 2 * 32 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32] = A[blockIdx_y // 2 * 32 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32]
                                        for ax0_ax1_fused_0 in T.serial(32):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 32 + ax0_ax1_fused_0, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + (threadIdx_y * 256 + threadIdx_x * 8 + ax0_ax1_fused_3)])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0, threadIdx_y * 256 + threadIdx_x * 8 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0, threadIdx_y * 256 + threadIdx_x * 8 + ax0_ax1_fused_3] = B[ax2_0_0 * 32 + ax0_ax1_fused_0, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + (threadIdx_y * 256 + threadIdx_x * 8 + ax0_ax1_fused_3)]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 32, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y * 256 : threadIdx_y * 256 + 256], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 256])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 256])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 256], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0 in T.serial(2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax1_0 in T.serial(16):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y * 256 + ax1_0 * 16 : threadIdx_y * 256 + ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 8320 + threadIdx_y * 256 + ax1_0 * 16, 8320, 1, dtype="handle"), 520, "row_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3, ax1_0_4 in T.grid(2, 8, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 16 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, A_reindex_shared_wmma_matrix_a.data, ax0_0_3, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 2 + ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 16 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 16):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, threadIdx_y * 256 + ax1_0 * 16 : threadIdx_y * 256 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 16 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 8192 + threadIdx_y * 256 + ax1_0 * 16, 8192, 2, dtype="handle"), 512, "row_major", dtype="handle"))
                            for ax0, ax1_0 in T.grid(32, 8):
                                with T.block("C_reindex_shared"):
                                    T.reads(C_reindex_shared[ax0, ax1_0 * 64 + threadIdx_y * 32 + threadIdx_x])
                                    T.writes(C[blockIdx_y // 2 * 32 + ax0, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + ax1_0 * 64 + threadIdx_y * 32 + threadIdx_x])
                                    C[blockIdx_y // 2 * 32 + ax0, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + ax1_0 * 64 + threadIdx_y * 32 + threadIdx_x] = C_reindex_shared[ax0, ax1_0 * 64 + threadIdx_y * 32 + threadIdx_x]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(8, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 : blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 : blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + 64])
                            T.writes(C[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 : blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + 256, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 : blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + 64])
                            C_reindex_shared = T.alloc_buffer([256, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 : blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 : blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + 64])
                                T.writes(C_reindex_shared[threadIdx_y * 128 : threadIdx_y * 128 + 128, 0 : 64])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([128, 64], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init, ax0_0_4_init in T.grid(4, 4, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 32 + ax0_0_4_init * 16 : ax0_0_3_init * 32 + ax0_0_4_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 8 + ax0_0_4_init * 4 + ax1_0_3_init * 16 // 16, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 : blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + 256, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 : blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 128, 0 : 64])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 128, 0 : 64])
                                        A_reindex_shared = T.alloc_buffer([256, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(32):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, ax2_0_0 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64] = A[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, ax2_0_0 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64]
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64] = B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64]
                                        for ax2_0_1 in T.serial(4):
                                            with T.block():
                                                T.reads(A_reindex_shared[threadIdx_y * 128 : threadIdx_y * 128 + 128, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, 0 : 64], C_reindex_shared_wmma_accumulator[0 : 128, 0 : 64])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 128, 0 : 64])
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([128, 16], dtype="float16", scope="wmma.matrix_a")
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 64], dtype="float16", scope="wmma.matrix_b")
                                                for ax0_0 in T.serial(8):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[threadIdx_y * 128 + ax0_0 * 16 : threadIdx_y * 128 + ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 9216 + ax0_0 * 1152 + ax2_0_1 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                                for ax1_0 in T.serial(4):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3, ax0_0_4 in T.grid(4, 4, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 8 + ax0_0_4 * 4 + ax1_0_3 * 16 // 16, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax0_0_4, B_reindex_shared_wmma_matrix_b.data, ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 8 + ax0_0_4 * 4 + ax1_0_3 * 16 // 16, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(8, 4):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 128 + ax0_0 * 16 : threadIdx_y * 128 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 8192 + ax0_0 * 1024 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(256):
                                with T.block("C_reindex_shared"):
                                    T.reads(C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x])
                                    T.writes(C[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + threadIdx_y * 32 + threadIdx_x])
                                    C[blockIdx_y // 4 * 1024 + blockIdx_x // 8 * 256 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 8 * 64 + threadIdx_y * 32 + threadIdx_x] = C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(64, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(8, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_x * 256 : blockIdx_x * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y * 32 : blockIdx_y * 32 + 32])
                            T.writes(C[blockIdx_x * 256 : blockIdx_x * 256 + 256, blockIdx_y * 32 : blockIdx_y * 32 + 128])
                            C_reindex_shared = T.alloc_buffer([256, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_x * 256 : blockIdx_x * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y * 32 : blockIdx_y * 32 + 32])
                                T.writes(C_reindex_shared[0 : 256, 0 : 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([256, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init, ax0_0_4_init in T.grid(8, 2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 32 + ax0_0_4_init * 16 : ax0_0_3_init * 32 + ax0_0_4_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 4 + ax0_0_4_init * 2 + ax1_0_3_init * 16 // 16, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_x * 256 : blockIdx_x * 256 + 256, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_y * 32 : blockIdx_y * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                        A_reindex_shared = T.alloc_buffer([256, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(256):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_x * 256 + ax0_ax1_fused_0, ax2_0_0 * 64 + (threadIdx_x * 2 + ax0_ax1_fused_3)])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0, threadIdx_x * 2 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0, threadIdx_x * 2 + ax0_ax1_fused_3] = A[blockIdx_x * 256 + ax0_ax1_fused_0, ax2_0_0 * 64 + (threadIdx_x * 2 + ax0_ax1_fused_3)]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_x // 8, blockIdx_y * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_x // 8, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_x // 8, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32] = B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_x // 8, blockIdx_y * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 256, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], B_reindex_shared[ax2_0_1 * 32 : ax2_0_1 * 32 + 32, 0 : 32], C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([256, 32], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0, ax1_0 in T.grid(16, 2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 1152 + ax2_0_1 * 32 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 32 + ax0_0 * 16 : ax2_0_1 * 32 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 1280 + ax0_0 * 640 + ax1_0 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3, ax2_0_2, ax0_0_4 in T.grid(8, 2, 2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax0_0_4 * 2 + ax1_0_3 * 16 // 16, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 4 + ax0_0_4 * 2 + ax2_0_2 * 16 // 16, B_reindex_shared_wmma_matrix_b.data, ax2_0_2 * 2 + ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax0_0_4 * 2 + ax1_0_3 * 16 // 16, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(16, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + ax1_0 * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(256):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 4 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_x * 256 + ax0, blockIdx_y * 32 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_x * 256 + ax0, blockIdx_y * 32 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(2, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(2048, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 1024 + blockIdx_x // 32 * 16 : blockIdx_y * 1024 + blockIdx_x // 32 * 16 + 16, 0 : 512], B[0 : 512, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 64])
                            T.writes(C[blockIdx_y * 1024 + blockIdx_x // 32 * 16 : blockIdx_y * 1024 + blockIdx_x // 32 * 16 + 16, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 256])
                            C_reindex_shared = T.alloc_buffer([16, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 1024 + blockIdx_x // 32 * 16 : blockIdx_y * 1024 + blockIdx_x // 32 * 16 + 16, 0 : 512], B[0 : 512, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 64])
                                T.writes(C_reindex_shared[0 : 16, threadIdx_y * 32 : threadIdx_y * 32 + 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([16, 32], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init in T.serial(2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 1024 + blockIdx_x // 32 * 16 : blockIdx_y * 1024 + blockIdx_x // 32 * 16 + 16, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 32])
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 64], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared = T.alloc_buffer([16, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([16, 64], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y * 1024 + blockIdx_x // 32 * 16 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 64 + (threadIdx_x * 2 + ax0_ax1_fused_3)])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x * 2 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x * 2 + ax0_ax1_fused_3] = A[blockIdx_y * 1024 + blockIdx_x // 32 * 16 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 64 + (threadIdx_x * 2 + ax0_ax1_fused_3)]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64, blockIdx_x % 32 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64] = B[ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64, blockIdx_x % 32 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64]
                                        for ax1_0 in T.serial(4):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0, ax1_0 in T.grid(2, 4):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, threadIdx_y * 2304 + ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "col_major", dtype="handle"))
                                        for ax1_0_3, ax2_0_2 in T.grid(2, 4):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[0 : 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax1_0_3 * 16 : ax1_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 4 + ax2_0_2, C_reindex_shared_wmma_accumulator.data, ax1_0_3, dtype="handle"))
                                for ax1_0 in T.serial(2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[0 : 16, threadIdx_y * 32 + ax1_0 * 16 : threadIdx_y * 32 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 32 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(16):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 128 + threadIdx_x * 4 + ax1_3 < 64)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y * 1024 + blockIdx_x // 32 * 16 + ax0, threadIdx_y * 128 + blockIdx_x % 32 * 64 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y * 1024 + blockIdx_x // 32 * 16 + ax0, threadIdx_y * 128 + blockIdx_x % 32 * 64 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(256, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(2, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 32 * 64 + blockIdx_x * 32 : blockIdx_y % 32 * 64 + blockIdx_x * 32 + 32])
                            T.writes(C[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, blockIdx_y % 32 * 64 + blockIdx_x * 32 : blockIdx_y % 32 * 64 + blockIdx_x * 32 + 256])
                            C_reindex_shared = T.alloc_buffer([256, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 32 * 64 + blockIdx_x * 32 : blockIdx_y % 32 * 64 + blockIdx_x * 32 + 32])
                                T.writes(C_reindex_shared[threadIdx_y * 128 : threadIdx_y * 128 + 128, 0 : 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([128, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_4_init in T.grid(8, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_4_init * 16 : ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_y % 32 * 64 + blockIdx_x * 32 : blockIdx_y % 32 * 64 + blockIdx_x * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 128, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 128, 0 : 32])
                                        B_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([128, 64], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([64, 32], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared = T.alloc_buffer([256, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 32 * 256 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 64 + (threadIdx_x * 2 + ax0_ax1_fused_3)])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x * 2 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x * 2 + ax0_ax1_fused_3] = A[blockIdx_y // 32 * 256 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 64 + (threadIdx_x * 2 + ax0_ax1_fused_3)]
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, blockIdx_y % 32 * 64 + blockIdx_x * 32 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 32])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, (threadIdx_x * 8 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, (threadIdx_x * 8 + ax0_ax1_fused_3) % 32] = B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 32, blockIdx_y % 32 * 64 + blockIdx_x * 32 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 32]
                                        for ax0_0, ax1_0 in T.grid(8, 4):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[threadIdx_y * 128 + ax0_0 * 16 : threadIdx_y * 128 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 9216 + ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0, ax1_0 in T.grid(4, 2):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 640 + ax1_0 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax0_0_3, ax2_0_2, ax1_0_4 in T.grid(8, 4, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 4 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax2_0_2 * 2 + ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_4, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(8, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 128 + ax0_0 * 16 : threadIdx_y * 128 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 4096 + ax0_0 * 512 + ax1_0 * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(256):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 128 + threadIdx_x * 4 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y // 32 * 256 + ax0, threadIdx_y * 128 + blockIdx_y % 32 * 64 + blockIdx_x * 32 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y // 32 * 256 + ax0, threadIdx_y * 128 + blockIdx_y % 32 * 64 + blockIdx_x * 32 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(16, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 : blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32])
                            T.writes(C[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 : blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + 256, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 256])
                            C_reindex_shared = T.alloc_buffer([256, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 : blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32])
                                T.writes(C_reindex_shared[0 : 256, 0 : 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([256, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax0_0_4_init, ax1_0_4_init in T.grid(8, 2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 32 + ax0_0_4_init * 16 : ax0_0_3_init * 32 + ax0_0_4_init * 16 + 16, ax1_0_4_init * 16 : ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 4 + ax0_0_4_init * 2 + ax1_0_4_init * 16 // 16, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 : blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + 256, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                        B_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([256, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + ax0_ax1_fused_0 * 2 + threadIdx_x // 16, ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_x // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_x // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64] = A[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + ax0_ax1_fused_0 * 2 + threadIdx_x // 16, ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64]
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 64 + ax0_ax1_fused_0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0, threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0, threadIdx_x] = B[ax2_0_0 * 64 + ax0_ax1_fused_0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x]
                                        for ax2_0_1 in T.serial(4):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 256, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, 0 : 32], C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 256, 0 : 32])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 32], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([256, 16], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0 in T.serial(16):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 1152 + ax2_0_1 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                                for ax1_0 in T.serial(2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 640 + ax1_0 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax0_0_3, ax0_0_4, ax1_0_4 in T.grid(8, 2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax0_0_4 * 2 + ax1_0_4 * 16 // 16, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax0_0_4, B_reindex_shared_wmma_matrix_b.data, ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax0_0_4 * 2 + ax1_0_4 * 16 // 16, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(16, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + ax1_0 * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(256):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 8 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y // 4 * 512 + blockIdx_x // 16 * 256 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(4, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(512, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_x // 8 * 32 : blockIdx_x // 8 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y * 512 + blockIdx_x % 8 * 64 : blockIdx_y * 512 + blockIdx_x % 8 * 64 + 64])
                            T.writes(C[blockIdx_x // 8 * 32 : blockIdx_x // 8 * 32 + 32, blockIdx_y * 512 + blockIdx_x % 8 * 64 : blockIdx_y * 512 + blockIdx_x % 8 * 64 + 256])
                            C_reindex_shared = T.alloc_buffer([32, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_x // 8 * 32 : blockIdx_x // 8 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y * 512 + blockIdx_x % 8 * 64 : blockIdx_y * 512 + blockIdx_x % 8 * 64 + 64])
                                T.writes(C_reindex_shared[0 : 32, threadIdx_y * 32 : threadIdx_y * 32 + 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_x // 8 * 32 : blockIdx_x // 8 * 32 + 32, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_y * 512 + blockIdx_x % 8 * 64 : blockIdx_y * 512 + blockIdx_x % 8 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        B_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([32, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 64], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 64], dtype="float16", scope="wmma.matrix_b")
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_x // 8 * 32 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64] = A[blockIdx_x // 8 * 32 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64, blockIdx_y * 512 + blockIdx_x % 8 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64] = B[ax2_0_0 * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64, blockIdx_y * 512 + blockIdx_x % 8 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64]
                                        for ax0_0, ax1_0 in T.grid(2, 4):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0, ax1_0 in T.grid(2, 4):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, threadIdx_y * 2304 + ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "col_major", dtype="handle"))
                                        for ax0_0_3, ax1_0_3, ax2_0_2 in T.grid(2, 2, 4):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax1_0_3 * 16 : ax1_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 4 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 4 + ax2_0_2, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, threadIdx_y * 32 + ax1_0 * 16 : threadIdx_y * 32 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 1024 + threadIdx_y * 32 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 128 + threadIdx_x * 4 + ax1_3 < 64)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_x // 8 * 32 + ax0, blockIdx_y * 512 + threadIdx_y * 128 + blockIdx_x % 8 * 64 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_x // 8 * 32 + ax0, blockIdx_y * 512 + threadIdx_y * 128 + blockIdx_x % 8 * 64 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(32, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 : blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + 64])
                            T.writes(C[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + 64, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 : blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + 64])
                            C_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 : blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + 64])
                                T.writes(C_reindex_shared[0 : 64, 0 : 64])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([64, 64], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init, ax0_0_4_init, ax1_0_4_init in T.grid(2, 4, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, ax1_0_3_init * 32 + ax1_0_4_init * 16 : ax1_0_3_init * 32 + ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init * 4 + (ax1_0_3_init * 32 + ax1_0_4_init * 16) // 16, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + 64, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 : blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([64, 64], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([64, 64], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + ax0_ax1_fused_0 // 2, ax2_0_0 * 64 + ax0_ax1_fused_0 % 2 * 32 + threadIdx_x])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 // 2, ax0_ax1_fused_0 % 2 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 // 2, ax0_ax1_fused_0 % 2 * 32 + threadIdx_x] = A[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + ax0_ax1_fused_0 // 2, ax2_0_0 * 64 + ax0_ax1_fused_0 % 2 * 32 + threadIdx_x]
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 64 + ax0_ax1_fused_0 // 2, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + ax0_ax1_fused_0 % 2 * 32 + threadIdx_x])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 // 2, ax0_ax1_fused_0 % 2 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 // 2, ax0_ax1_fused_0 % 2 * 32 + threadIdx_x] = B[ax2_0_0 * 64 + ax0_ax1_fused_0 // 2, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + ax0_ax1_fused_0 % 2 * 32 + threadIdx_x]
                                        for ax0_0, ax1_0 in T.grid(4, 4):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0, ax1_0 in T.grid(4, 4):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax1_0_3, ax2_0_2, ax0_0_4, ax1_0_4 in T.grid(2, 4, 4, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, A_reindex_shared_wmma_matrix_a.data, ax0_0_4 * 4 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax2_0_2 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(4, 4):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 1024 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                for ax1_3 in T.vectorized(2):
                                    with T.block("C_reindex_shared"):
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 2 + ax1_3])
                                        T.writes(C[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + ax0, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + threadIdx_x * 2 + ax1_3])
                                        C[blockIdx_y // 8 * 512 + blockIdx_x // 4 * 64 + ax0, blockIdx_y % 8 * 256 + blockIdx_x % 4 * 64 + threadIdx_x * 2 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 2 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(256, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(16, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 : blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 : blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + 16])
                            T.writes(C[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 : blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + 64, blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 : blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + 128])
                            C_reindex_shared = T.alloc_buffer([64, 16], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 : blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 : blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + 16])
                                T.writes(C_reindex_shared[threadIdx_y * 32 : threadIdx_y * 32 + 32, 0 : 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_4_init in T.serial(2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(32):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 : blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + 64, ax2_0_0 * 16 : ax2_0_0 * 16 + 16], B[ax2_0_0 * 16 : ax2_0_0 * 16 + 16, blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 : blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        B_reindex_shared = T.alloc_buffer([16, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared = T.alloc_buffer([64, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16, ax2_0_0 * 16 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 16])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16, (threadIdx_x * 4 + ax0_ax1_fused_3) % 16] = A[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + ax0_ax1_fused_0 * 16 + threadIdx_y * 8 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 16, ax2_0_0 * 16 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 16]
                                        for ax0_ax1_fused_0 in T.serial(2):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16] = B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 16, blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16]
                                        for ax0_0 in T.serial(2):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, 0 : 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 1280 + ax0_0 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                            T.reads(B_reindex_shared[0 : 16, 0 : 16])
                                            T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                            T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, 0, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax0_0_4 in T.serial(2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_4, B_reindex_shared_wmma_matrix_b.data, 0, C_reindex_shared_wmma_accumulator.data, ax0_0_4, dtype="handle"))
                                for ax0_0 in T.serial(2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, 0 : 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 512 + ax0_0 * 256, 256, 2, dtype="handle"), 16, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                for ax1_3 in T.vectorized(2):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 64 + threadIdx_x * 2 + ax1_3 < 16)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 64 + threadIdx_x * 2 + ax1_3])
                                        T.writes(C[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + ax0, threadIdx_y * 64 + blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + threadIdx_x * 2 + ax1_3])
                                        C[blockIdx_y // 64 * 512 + blockIdx_x // 2 * 64 + ax0, threadIdx_y * 64 + blockIdx_y % 64 * 32 + blockIdx_x % 2 * 16 + threadIdx_x * 2 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 64 + threadIdx_x * 2 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(32, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(8, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(4, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 : blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 128])
                            T.writes(C[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 : blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + 128, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 128])
                            C_reindex_shared = T.alloc_buffer([128, 128], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 : blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 128])
                                T.writes(C_reindex_shared[threadIdx_y // 2 * 64 : threadIdx_y // 2 * 64 + 64, threadIdx_y % 2 * 64 : threadIdx_y % 2 * 64 + 64])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([64, 64], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init, ax0_0_4_init, ax1_0_4_init in T.grid(2, 2, 2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 32 + ax0_0_4_init * 16 : ax0_0_3_init * 32 + ax0_0_4_init * 16 + 16, ax1_0_3_init * 32 + ax1_0_4_init * 16 : ax1_0_3_init * 32 + ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 8 + ax0_0_4_init * 4 + (ax1_0_3_init * 32 + ax1_0_4_init * 16) // 16, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 : blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + 128, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 128], C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                        B_reindex_shared = T.alloc_buffer([32, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([128, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32] = A[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128] = B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[threadIdx_y // 2 * 64 : threadIdx_y // 2 * 64 + 64, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y % 2 * 64 : threadIdx_y % 2 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 64, 0 : 64])
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([64, 16], dtype="float16", scope="wmma.matrix_a")
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 64], dtype="float16", scope="wmma.matrix_b")
                                                for ax0_0 in T.serial(4):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[threadIdx_y // 2 * 64 + ax0_0 * 16 : threadIdx_y // 2 * 64 + ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y // 2 * 2560 + ax0_0 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax1_0 in T.serial(4):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y % 2 * 64 + ax1_0 * 16 : threadIdx_y % 2 * 64 + ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 2176 + threadIdx_y % 2 * 64 + ax1_0 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3, ax0_0_4, ax1_0_4 in T.grid(2, 2, 2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 8 + ax0_0_4 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax0_0_4, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 2 + ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 8 + ax0_0_4 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(4, 4):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y // 2 * 64 + ax0_0 * 16 : threadIdx_y // 2 * 64 + ax0_0 * 16 + 16, threadIdx_y % 2 * 64 + ax1_0 * 16 : threadIdx_y % 2 * 64 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y // 2 * 8192 + ax0_0 * 2048 + threadIdx_y % 2 * 64 + ax1_0 * 16, 2048, 2, dtype="handle"), 128, "row_major", dtype="handle"))
                            for ax0 in T.serial(128):
                                with T.block("C_reindex_shared"):
                                    T.reads(C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x])
                                    T.writes(C[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + threadIdx_y * 32 + threadIdx_x])
                                    C[blockIdx_y // 4 * 256 + blockIdx_x // 4 * 128 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + threadIdx_y * 32 + threadIdx_x] = C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(16, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(256, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 128 + blockIdx_x // 64 * 32 : blockIdx_y * 128 + blockIdx_x // 64 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 32])
                            T.writes(C[blockIdx_y * 128 + blockIdx_x // 64 * 32 : blockIdx_y * 128 + blockIdx_x // 64 * 32 + 32, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 128])
                            C_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 128 + blockIdx_x // 64 * 32 : blockIdx_y * 128 + blockIdx_x // 64 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 32])
                                T.writes(C_reindex_shared[0 : 32, 0 : 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(32):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 128 + blockIdx_x // 64 * 32 : blockIdx_y * 128 + blockIdx_x // 64 * 32 + 32, ax2_0_0 * 16 : ax2_0_0 * 16 + 16], B[ax2_0_0 * 16 : ax2_0_0 * 16 + 16, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        A_reindex_shared = T.alloc_buffer([32, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 32], dtype="float16", scope="wmma.matrix_b")
                                        B_reindex_shared = T.alloc_buffer([16, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(2):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y * 128 + blockIdx_x // 64 * 32 + ax0_ax1_fused_0 * 16 + threadIdx_x // 2, ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_x // 2, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_x // 2, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16] = A[blockIdx_y * 128 + blockIdx_x // 64 * 32 + ax0_ax1_fused_0 * 16 + threadIdx_x // 2, ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16]
                                        for ax0_ax1_fused_0 in T.serial(2):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 8 + threadIdx_x // 4, blockIdx_x % 64 * 32 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 32])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_x // 4, (threadIdx_x * 8 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_x // 4, (threadIdx_x * 8 + ax0_ax1_fused_3) % 32] = B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 8 + threadIdx_x // 4, blockIdx_x % 64 * 32 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 32]
                                        for ax0_0 in T.serial(2):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax1_0 in T.serial(2):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax1_0 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax0_0_3, ax1_0_3 in T.grid(2, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3, B_reindex_shared_wmma_matrix_b.data, ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + ax1_0 * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 4 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y * 128 + blockIdx_x // 64 * 32 + ax0, blockIdx_x % 64 * 32 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y * 128 + blockIdx_x // 64 * 32 + ax0, blockIdx_x % 64 * 32 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(256, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(4, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(4, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 32 * 256 + blockIdx_x * 64 : blockIdx_y // 32 * 256 + blockIdx_x * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 64])
                            T.writes(C[blockIdx_y // 32 * 256 + blockIdx_x * 64 : blockIdx_y // 32 * 256 + blockIdx_x * 64 + 64, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 128])
                            C_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 32 * 256 + blockIdx_x * 64 : blockIdx_y // 32 * 256 + blockIdx_x * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 64])
                                T.writes(C_reindex_shared[threadIdx_y // 2 * 32 : threadIdx_y // 2 * 32 + 32, threadIdx_y % 2 * 32 : threadIdx_y % 2 * 32 + 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(32):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 32 * 256 + blockIdx_x * 64 : blockIdx_y // 32 * 256 + blockIdx_x * 64 + 64, ax2_0_0 * 16 : ax2_0_0 * 16 + 16], B[ax2_0_0 * 16 : ax2_0_0 * 16 + 16, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 32], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared = T.alloc_buffer([16, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([64, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y // 32 * 256 + blockIdx_x * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + threadIdx_x // 16, ax2_0_0 * 16 + threadIdx_x % 16])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + threadIdx_x // 16, threadIdx_x % 16])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + threadIdx_x // 16, threadIdx_x % 16] = A[blockIdx_y // 32 * 256 + blockIdx_x * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + threadIdx_x // 16, ax2_0_0 * 16 + threadIdx_x % 16]
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 2 + threadIdx_y // 2, blockIdx_y % 32 * 64 + (threadIdx_y * 32 + threadIdx_x) % 64])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y // 2, (threadIdx_y * 32 + threadIdx_x) % 64])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y // 2, (threadIdx_y * 32 + threadIdx_x) % 64] = B[ax2_0_0 * 16 + ax0_ax1_fused_0 * 2 + threadIdx_y // 2, blockIdx_y % 32 * 64 + (threadIdx_y * 32 + threadIdx_x) % 64]
                                        for ax0_0 in T.serial(2):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[threadIdx_y // 2 * 32 + ax0_0 * 16 : threadIdx_y // 2 * 32 + ax0_0 * 16 + 16, 0 : 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y // 2 * 1280 + ax0_0 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax1_0 in T.serial(2):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[0 : 16, threadIdx_y % 2 * 32 + ax1_0 * 16 : threadIdx_y % 2 * 32 + ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, threadIdx_y % 2 * 32 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0_3, ax1_0_3 in T.grid(2, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3, B_reindex_shared_wmma_matrix_b.data, ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y // 2 * 32 + ax0_0 * 16 : threadIdx_y // 2 * 32 + ax0_0 * 16 + 16, threadIdx_y % 2 * 32 + ax1_0 * 16 : threadIdx_y % 2 * 32 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y // 2 * 2048 + ax0_0 * 1024 + threadIdx_y % 2 * 32 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                with T.block("C_reindex_shared"):
                                    T.where(threadIdx_y * 32 + threadIdx_x < 64)
                                    T.reads(C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x])
                                    T.writes(C[blockIdx_y // 32 * 256 + blockIdx_x * 64 + ax0, blockIdx_y % 32 * 64 + threadIdx_y * 32 + threadIdx_x])
                                    C[blockIdx_y // 32 * 256 + blockIdx_x * 64 + ax0, blockIdx_y % 32 * 64 + threadIdx_y * 32 + threadIdx_x] = C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(4096, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(1, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 64 * 32 : blockIdx_y // 64 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 64 * 32 : blockIdx_y % 64 * 32 + 32])
                            T.writes(C[blockIdx_y // 64 * 32 : blockIdx_y // 64 * 32 + 32, blockIdx_y % 64 * 32 : blockIdx_y % 64 * 32 + 128])
                            C_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 64 * 32 : blockIdx_y // 64 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 64 * 32 : blockIdx_y % 64 * 32 + 32])
                                T.writes(C_reindex_shared[0 : 32, 0 : 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_4_init, ax1_0_4_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, ax1_0_4_init * 16 : ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init * 2 + ax1_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(4):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 64 * 32 : blockIdx_y // 64 * 32 + 32, ax2_0_0 * 128 : ax2_0_0 * 128 + 128], B[ax2_0_0 * 128 : ax2_0_0 * 128 + 128, blockIdx_y % 64 * 32 : blockIdx_y % 64 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        B_reindex_shared = T.alloc_buffer([32, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([32, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y // 64 * 32 + ax0_ax1_fused_0 // 4, ax2_0_0 * 128 + ax0_ax1_fused_0 % 4 * 32 + threadIdx_x])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 // 4, ax0_ax1_fused_0 % 4 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 // 4, ax0_ax1_fused_0 % 4 * 32 + threadIdx_x] = A[blockIdx_y // 64 * 32 + ax0_ax1_fused_0 // 4, ax2_0_0 * 128 + ax0_ax1_fused_0 % 4 * 32 + threadIdx_x]
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 128 + (ax0_ax1_fused_0 * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128, blockIdx_y % 64 * 32 + ax0_ax1_fused_0 // 2])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 // 2, (ax0_ax1_fused_0 * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 // 2, (ax0_ax1_fused_0 * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128] = B[ax2_0_0 * 128 + (ax0_ax1_fused_0 * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128, blockIdx_y % 64 * 32 + ax0_ax1_fused_0 // 2]
                                        for ax2_0_1 in T.serial(8):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 32, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[0 : 32, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0 in T.serial(2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 2176 + ax2_0_1 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                                for ax0_0 in T.serial(2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 2176 + ax2_0_1 * 16, 2176, 1, dtype="handle"), 136, "col_major", dtype="handle"))
                                                for ax0_0_4, ax1_0_4 in T.grid(2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[ax1_0_4 * 16 : ax1_0_4 * 16 + 16, 0 : 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 2 + ax1_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_4, B_reindex_shared_wmma_matrix_b.data, ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 2 + ax1_0_4, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + ax1_0 * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 4 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y // 64 * 32 + ax0, blockIdx_y % 64 * 32 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y // 64 * 32 + ax0, blockIdx_y % 64 * 32 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(256, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(1, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 64])
                            T.writes(C[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 64])
                            C_reindex_shared = T.alloc_buffer([256, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 64])
                                T.writes(C_reindex_shared[0 : 256, 0 : 64])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([256, 64], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init, ax0_0_4_init, ax1_0_4_init in T.grid(8, 2, 2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 32 + ax0_0_4_init * 16 : ax0_0_3_init * 32 + ax0_0_4_init * 16 + 16, ax1_0_3_init * 32 + ax1_0_4_init * 16 : ax1_0_3_init * 32 + ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 8 + ax0_0_4_init * 4 + (ax1_0_3_init * 32 + ax1_0_4_init * 16) // 16, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 32 * 256 : blockIdx_y // 32 * 256 + 256, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y % 32 * 64 : blockIdx_y % 32 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 256, 0 : 64])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 256, 0 : 64])
                                        A_reindex_shared = T.alloc_buffer([256, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([256, 32], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared = T.alloc_buffer([32, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 64], dtype="float16", scope="wmma.matrix_b")
                                        for ax0_ax1_fused_0 in T.serial(256):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y // 32 * 256 + ax0_ax1_fused_0, ax2_0_0 * 32 + threadIdx_x])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0, threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0, threadIdx_x] = A[blockIdx_y // 32 * 256 + ax0_ax1_fused_0, ax2_0_0 * 32 + threadIdx_x]
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 32 + ax0_ax1_fused_0 // 2, blockIdx_y % 32 * 64 + ax0_ax1_fused_0 % 2 * 32 + threadIdx_x])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 // 2, ax0_ax1_fused_0 % 2 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 // 2, ax0_ax1_fused_0 % 2 * 32 + threadIdx_x] = B[ax2_0_0 * 32 + ax0_ax1_fused_0 // 2, blockIdx_y % 32 * 64 + ax0_ax1_fused_0 % 2 * 32 + threadIdx_x]
                                        for ax0_0, ax1_0 in T.grid(16, 2):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 640 + ax1_0 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax0_0, ax1_0 in T.grid(2, 4):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0_3, ax1_0_3, ax2_0_2, ax0_0_4, ax1_0_4 in T.grid(8, 2, 2, 2, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 8 + ax0_0_4 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 4 + ax0_0_4 * 2 + ax2_0_2 * 16 // 16, B_reindex_shared_wmma_matrix_b.data, ax2_0_2 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 8 + ax0_0_4 * 4 + (ax1_0_3 * 32 + ax1_0_4 * 16) // 16, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(16, 4):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 1024 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(256):
                                for ax1_3 in T.vectorized(2):
                                    with T.block("C_reindex_shared"):
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 2 + ax1_3])
                                        T.writes(C[blockIdx_y // 32 * 256 + ax0, blockIdx_y % 32 * 64 + threadIdx_x * 2 + ax1_3])
                                        C[blockIdx_y // 32 * 256 + ax0, blockIdx_y % 32 * 64 + threadIdx_x * 2 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 2 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(8, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(512, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 256 + blockIdx_x // 64 * 32 : blockIdx_y * 256 + blockIdx_x // 64 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 32])
                            T.writes(C[blockIdx_y * 256 + blockIdx_x // 64 * 32 : blockIdx_y * 256 + blockIdx_x // 64 * 32 + 32, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 128])
                            C_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 256 + blockIdx_x // 64 * 32 : blockIdx_y * 256 + blockIdx_x // 64 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 32])
                                T.writes(C_reindex_shared[0 : 32, threadIdx_y * 16 : threadIdx_y * 16 + 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init in T.serial(2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 256 + blockIdx_x // 64 * 32 : blockIdx_y * 256 + blockIdx_x // 64 * 32 + 32, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_x % 64 * 32 : blockIdx_x % 64 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 16])
                                        A_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 32], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_a")
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y * 256 + blockIdx_x // 64 * 32 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 32])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 32, (threadIdx_x * 2 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 32, (threadIdx_x * 2 + ax0_ax1_fused_3) % 32] = A[blockIdx_y * 256 + blockIdx_x // 64 * 32 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 2 + ax0_ax1_fused_3) // 32, ax2_0_0 * 32 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 32]
                                        for ax0_ax1_fused_0 in T.serial(4):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32, blockIdx_x % 64 * 32 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32] = B[ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32, blockIdx_x % 64 * 32 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 32]
                                        for ax0_0, ax1_0 in T.grid(2, 2):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 640 + ax1_0 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax1_0 in T.serial(2):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, threadIdx_y * 640 + ax1_0 * 16, 640, 1, dtype="handle"), 40, "col_major", dtype="handle"))
                                        for ax0_0_3, ax2_0_2 in T.grid(2, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, 0 : 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax2_0_2, C_reindex_shared_wmma_accumulator.data, ax0_0_3, dtype="handle"))
                                for ax0_0 in T.serial(2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, threadIdx_y * 16 : threadIdx_y * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + threadIdx_y * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(2):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 64 + threadIdx_x * 2 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 64 + threadIdx_x * 2 + ax1_3])
                                        T.writes(C[blockIdx_y * 256 + blockIdx_x // 64 * 32 + ax0, threadIdx_y * 64 + blockIdx_x % 64 * 32 + threadIdx_x * 2 + ax1_3])
                                        C[blockIdx_y * 256 + blockIdx_x // 64 * 32 + ax0, threadIdx_y * 64 + blockIdx_x % 64 * 32 + threadIdx_x * 2 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 64 + threadIdx_x * 2 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(16, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(128, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 : blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 : blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + 16])
                            T.writes(C[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 : blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + 128, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 : blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + 256])
                            C_reindex_shared = T.alloc_buffer([128, 16], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 : blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 : blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + 16])
                                T.writes(C_reindex_shared[0 : 128, 0 : 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([128, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax0_0_4_init in T.grid(2, 4):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 64 + ax0_0_4_init * 16 : ax0_0_3_init * 64 + ax0_0_4_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 4 + ax0_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(4):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 : blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + 128, ax2_0_0 * 128 : ax2_0_0 * 128 + 128], B[ax2_0_0 * 128 : ax2_0_0 * 128 + 128, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 : blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 128, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 128, 0 : 16])
                                        A_reindex_shared = T.alloc_buffer([128, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([128, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + ax0_ax1_fused_0, ax2_0_0 * 128 + (threadIdx_x * 4 + ax0_ax1_fused_3)])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0, threadIdx_x * 4 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0, threadIdx_x * 4 + ax0_ax1_fused_3] = A[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + ax0_ax1_fused_0, ax2_0_0 * 128 + (threadIdx_x * 4 + ax0_ax1_fused_3)]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 128 + ax0_ax1_fused_0 * 8 + threadIdx_x // 4, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 16])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_x // 4, (threadIdx_x * 4 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_x // 4, (threadIdx_x * 4 + ax0_ax1_fused_3) % 16] = B[ax2_0_0 * 128 + ax0_ax1_fused_0 * 8 + threadIdx_x // 4, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 16]
                                        for ax2_0_1 in T.serial(4):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 128, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], B_reindex_shared[ax2_0_1 * 32 : ax2_0_1 * 32 + 32, 0 : 16], C_reindex_shared_wmma_accumulator[0 : 128, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 128, 0 : 16])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([128, 32], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0, ax1_0 in T.grid(8, 2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 2176 + ax2_0_1 * 32 + ax1_0 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                                for ax0_0 in T.serial(2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 32 + ax0_0 * 16 : ax2_0_1 * 32 + ax0_0 * 16 + 16, 0 : 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 1280 + ax0_0 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax0_0_3, ax2_0_2, ax0_0_4 in T.grid(2, 2, 4):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 64 + ax0_0_4 * 16 : ax0_0_3 * 64 + ax0_0_4 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 64 + ax0_0_4 * 16 : ax0_0_3 * 64 + ax0_0_4 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, 0 : 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 64 + ax0_0_4 * 16 : ax0_0_3 * 64 + ax0_0_4 * 16 + 16, 0 : 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax0_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 8 + ax0_0_4 * 2 + ax2_0_2 * 16 // 16, B_reindex_shared_wmma_matrix_b.data, ax2_0_2, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax0_0_4, dtype="handle"))
                                for ax0_0 in T.serial(8):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 256, 256, 2, dtype="handle"), 16, "row_major", dtype="handle"))
                            for ax0 in T.serial(128):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 8 + ax1_3 < 16)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + ax0, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y // 2 * 256 + blockIdx_x // 64 * 128 + ax0, blockIdx_y % 2 * 1024 + blockIdx_x % 64 * 16 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(32, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_x // 4 * 256 : blockIdx_x // 4 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y * 64 + blockIdx_x % 4 * 16 : blockIdx_y * 64 + blockIdx_x % 4 * 16 + 16])
                            T.writes(C[blockIdx_x // 4 * 256 : blockIdx_x // 4 * 256 + 256, blockIdx_y * 64 + blockIdx_x % 4 * 16 : blockIdx_y * 64 + blockIdx_x % 4 * 16 + 256])
                            C_reindex_shared = T.alloc_buffer([256, 16], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_x // 4 * 256 : blockIdx_x // 4 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_y * 64 + blockIdx_x % 4 * 16 : blockIdx_y * 64 + blockIdx_x % 4 * 16 + 16])
                                T.writes(C_reindex_shared[0 : 256, 0 : 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([256, 16], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax0_0_4_init in T.grid(8, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 32 + ax0_0_4_init * 16 : ax0_0_3_init * 32 + ax0_0_4_init * 16 + 16, 0 : 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax0_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_x // 4 * 256 : blockIdx_x // 4 * 256 + 256, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y * 64 + blockIdx_x % 4 * 16 : blockIdx_y * 64 + blockIdx_x % 4 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 256, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 256, 0 : 16])
                                        B_reindex_shared = T.alloc_buffer([32, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([256, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(128):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_x // 4 * 256 + ax0_ax1_fused_0 * 2 + threadIdx_x // 16, ax2_0_0 * 32 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 32])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_x // 16, (threadIdx_x * 2 + ax0_ax1_fused_3) % 32])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_x // 16, (threadIdx_x * 2 + ax0_ax1_fused_3) % 32] = A[blockIdx_x // 4 * 256 + ax0_ax1_fused_0 * 2 + threadIdx_x // 16, ax2_0_0 * 32 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 32]
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 4 + threadIdx_x // 8, blockIdx_y * 64 + blockIdx_x % 4 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_x // 8, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_x // 8, (threadIdx_x * 2 + ax0_ax1_fused_3) % 16] = B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 4 + threadIdx_x // 8, blockIdx_y * 64 + blockIdx_x % 4 * 16 + (threadIdx_x * 2 + ax0_ax1_fused_3) % 16]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 256, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, 0 : 16], C_reindex_shared_wmma_accumulator[0 : 256, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 256, 0 : 16])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([256, 16], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0 in T.serial(16):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                    T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, 0 : 16])
                                                    T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                    T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax0_0_3, ax0_0_4 in T.grid(8, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 32 + ax0_0_4 * 16 : ax0_0_3 * 32 + ax0_0_4 * 16 + 16, 0 : 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax0_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax0_0_4, B_reindex_shared_wmma_matrix_b.data, 0, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax0_0_4, dtype="handle"))
                                for ax0_0 in T.serial(16):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 256, 256, 2, dtype="handle"), 16, "row_major", dtype="handle"))
                            for ax0 in T.serial(256):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 8 + ax1_3 < 16)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_x // 4 * 256 + ax0, blockIdx_y * 64 + blockIdx_x % 4 * 16 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_x // 4 * 256 + ax0, blockIdx_y * 64 + blockIdx_x % 4 * 16 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(1, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(1024, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(4, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_x // 16 * 32 : blockIdx_x // 16 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 128])
                            T.writes(C[blockIdx_x // 16 * 32 : blockIdx_x // 16 * 32 + 32, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 128])
                            C_reindex_shared = T.alloc_buffer([32, 128], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_x // 16 * 32 : blockIdx_x // 16 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 128])
                                T.writes(C_reindex_shared[0 : 32, threadIdx_y * 32 : threadIdx_y * 32 + 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(4):
                                    with T.block():
                                        T.reads(A[blockIdx_x // 16 * 32 : blockIdx_x // 16 * 32 + 32, ax2_0_0 * 128 : ax2_0_0 * 128 + 128], B[ax2_0_0 * 128 : ax2_0_0 * 128 + 128, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 128], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        A_reindex_shared = T.alloc_buffer([32, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([128, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(2):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_x // 16 * 32 + ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, ax2_0_0 * 128 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128] = A[blockIdx_x // 16 * 32 + ax0_ax1_fused_0 * 2 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) // 128, ax2_0_0 * 128 + (threadIdx_y * 64 + threadIdx_x * 2 + ax0_ax1_fused_3) % 128]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 128 + ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 128, blockIdx_x % 16 * 128 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 128])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 128, (threadIdx_x * 8 + ax0_ax1_fused_3) % 128])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 128, (threadIdx_x * 8 + ax0_ax1_fused_3) % 128] = B[ax2_0_0 * 128 + ax0_ax1_fused_0 * 8 + threadIdx_y * 2 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 128, blockIdx_x % 16 * 128 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 128]
                                        for ax2_0_1 in T.serial(4):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 32, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], B_reindex_shared[ax2_0_1 * 32 : ax2_0_1 * 32 + 32, threadIdx_y * 32 : threadIdx_y * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 2176 + ax2_0_1 * 32 + ax1_0 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 32 + ax0_0 * 16 : ax2_0_1 * 32 + ax0_0 * 16 + 16, threadIdx_y * 32 + ax1_0 * 16 : threadIdx_y * 32 + ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 4352 + ax0_0 * 2176 + threadIdx_y * 32 + ax1_0 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3, ax2_0_2 in T.grid(2, 2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax2_0_2 * 2 + ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, threadIdx_y * 32 + ax1_0 * 16 : threadIdx_y * 32 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 2048 + threadIdx_y * 32 + ax1_0 * 16, 2048, 2, dtype="handle"), 128, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                with T.block("C_reindex_shared"):
                                    T.reads(C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x])
                                    T.writes(C[blockIdx_x // 16 * 32 + ax0, blockIdx_x % 16 * 128 + threadIdx_y * 32 + threadIdx_x])
                                    C[blockIdx_x // 16 * 32 + ax0, blockIdx_x % 16 * 128 + threadIdx_y * 32 + threadIdx_x] = C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(1, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(256, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(4, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_x // 32 * 256 : blockIdx_x // 32 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 64])
                            T.writes(C[blockIdx_x // 32 * 256 : blockIdx_x // 32 * 256 + 256, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 256])
                            C_reindex_shared = T.alloc_buffer([256, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_x // 32 * 256 : blockIdx_x // 32 * 256 + 256, 0 : 512], B[0 : 512, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 64])
                                T.writes(C_reindex_shared[threadIdx_y // 2 * 128 : threadIdx_y // 2 * 128 + 128, threadIdx_y % 2 * 32 : threadIdx_y % 2 * 32 + 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([128, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init in T.grid(8, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_x // 32 * 256 : blockIdx_x // 32 * 256 + 256, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_x % 32 * 64 : blockIdx_x % 32 * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 128, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 128, 0 : 32])
                                        B_reindex_shared = T.alloc_buffer([32, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([256, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_x // 32 * 256 + ax0_ax1_fused_0 * 4 + threadIdx_y, ax2_0_0 * 32 + threadIdx_x])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y, threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y, threadIdx_x] = A[blockIdx_x // 32 * 256 + ax0_ax1_fused_0 * 4 + threadIdx_y, ax2_0_0 * 32 + threadIdx_x]
                                        for ax0_ax1_fused_0 in T.serial(2):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, blockIdx_x % 32 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64] = B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 16 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, blockIdx_x % 32 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[threadIdx_y // 2 * 128 : threadIdx_y // 2 * 128 + 128, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y % 2 * 32 : threadIdx_y % 2 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 128, 0 : 32])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 128, 0 : 32])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 32], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([128, 16], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0 in T.serial(8):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[threadIdx_y // 2 * 128 + ax0_0 * 16 : threadIdx_y // 2 * 128 + ax0_0 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y // 2 * 5120 + ax0_0 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax1_0 in T.serial(2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, threadIdx_y % 2 * 32 + ax1_0 * 16 : threadIdx_y % 2 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 1152 + threadIdx_y % 2 * 32 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3 in T.grid(8, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":T.int64(1024), "meta_schedule.thread_extent_low_inclusive":T.int64(32), "warp_execution":T.int64(1)})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3, B_reindex_shared_wmma_matrix_b.data, ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(8, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y // 2 * 128 + ax0_0 * 16 : threadIdx_y // 2 * 128 + ax0_0 * 16 + 16, threadIdx_y % 2 * 32 + ax1_0 * 16 : threadIdx_y % 2 * 32 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y // 2 * 8192 + ax0_0 * 1024 + threadIdx_y % 2 * 32 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(256):
                                for ax1_3 in T.vectorized(2):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 64 + threadIdx_x * 2 + ax1_3 < 64)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 64 + threadIdx_x * 2 + ax1_3])
                                        T.writes(C[blockIdx_x // 32 * 256 + ax0, threadIdx_y * 64 + blockIdx_x % 32 * 64 + threadIdx_x * 2 + ax1_3])
                                        C[blockIdx_x // 32 * 256 + ax0, threadIdx_y * 64 + blockIdx_x % 32 * 64 + threadIdx_x * 2 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 64 + threadIdx_x * 2 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(128, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(2, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512])
                            T.writes(C[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512])
                            C_reindex_shared = T.alloc_buffer([32, 512], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512])
                                T.writes(C_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, 0 : 512])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([16, 512], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init, ax1_0_4_init in T.grid(16, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3_init * 32 + ax1_0_4_init * 16 : ax1_0_3_init * 32 + ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0_3_init * 2 + ax1_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 2 * 32 : blockIdx_y // 2 * 32 + 32, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y % 2 * 1024 + blockIdx_x * 512 : blockIdx_y % 2 * 1024 + blockIdx_x * 512 + 512], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 512])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 512])
                                        A_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([32, 512], dtype="float16", strides=[520, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y // 2 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 32 + threadIdx_x])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x] = A[blockIdx_y // 2 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 32 + threadIdx_x]
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 32 + (ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) // 512, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + (ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 512])
                                                    T.writes(B_reindex_shared[(ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) // 512, (ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 512])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[(ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) // 512, (ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 512] = B[ax2_0_0 * 32 + (ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) // 512, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + (ax0_ax1_fused_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 512]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, 0 : 512], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 512])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 512])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 512], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_a")
                                                with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                    T.reads(A_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                    T.writes(A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16])
                                                    T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                for ax1_0 in T.serial(32):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax2_0_1 * 16 : ax2_0_1 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax2_0_1 * 8320 + ax1_0 * 16, 8320, 1, dtype="handle"), 520, "row_major", dtype="handle"))
                                                for ax1_0_3, ax1_0_4 in T.grid(16, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 32 + ax1_0_4 * 16 : ax1_0_3 * 32 + ax1_0_4 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax1_0_3 * 2 + ax1_0_4, A_reindex_shared_wmma_matrix_a.data, 0, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 2 + ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax1_0_3 * 2 + ax1_0_4, dtype="handle"))
                                for ax1_0 in T.serial(32):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 16 : threadIdx_y * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 8192 + ax1_0 * 16, 8192, 2, dtype="handle"), 512, "row_major", dtype="handle"))
                            for ax0, ax1_0 in T.grid(32, 2):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.reads(C_reindex_shared[ax0, ax1_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y // 2 * 32 + ax0, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + ax1_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y // 2 * 32 + ax0, blockIdx_y % 2 * 1024 + blockIdx_x * 512 + ax1_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, ax1_0 * 256 + threadIdx_y * 128 + threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(128, thread="blockIdx.y"):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(1, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32])
                            T.writes(C[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 256])
                            C_reindex_shared = T.alloc_buffer([32, 32], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32])
                                T.writes(C_reindex_shared[0 : 32, 0 : 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_3_init in T.grid(2, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 2 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(2):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 : blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + 32, ax2_0_0 * 256 : ax2_0_0 * 256 + 256], B[ax2_0_0 * 256 : ax2_0_0 * 256 + 256, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 : blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                        A_reindex_shared = T.alloc_buffer([32, 256], dtype="float16", strides=[264, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([32, 256], dtype="float16", strides=[264, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(64):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0_ax1_fused_0 // 2, ax2_0_0 * 256 + (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 // 2, (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 // 2, (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256] = A[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0_ax1_fused_0 // 2, ax2_0_0 * 256 + (ax0_ax1_fused_0 * 128 + threadIdx_x * 4 + ax0_ax1_fused_3) % 256]
                                        for ax0_ax1_fused_0 in T.serial(256):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 256 + ax0_ax1_fused_0 % 8 * 32 + threadIdx_x, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + ax0_ax1_fused_0 // 8])
                                                T.writes(B_reindex_shared[ax0_ax1_fused_0 // 8, ax0_ax1_fused_0 % 8 * 32 + threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[ax0_ax1_fused_0 // 8, ax0_ax1_fused_0 % 8 * 32 + threadIdx_x] = B[ax2_0_0 * 256 + ax0_ax1_fused_0 % 8 * 32 + threadIdx_x, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + ax0_ax1_fused_0 // 8]
                                        for ax2_0_1 in T.serial(8):
                                            with T.block():
                                                T.reads(A_reindex_shared[0 : 32, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], B_reindex_shared[0 : 32, ax2_0_1 * 32 : ax2_0_1 * 32 + 32], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 32])
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_b")
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_a")
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                        T.reads(A_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, ax0_0 * 4224 + ax2_0_1 * 32 + ax1_0 * 16, 4224, 1, dtype="handle"), 264, "row_major", dtype="handle"))
                                                for ax0_0, ax1_0 in T.grid(2, 2):
                                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                        T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax2_0_1 * 32 + ax1_0 * 16 : ax2_0_1 * 32 + ax1_0 * 16 + 16])
                                                        T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                        T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 4224 + ax2_0_1 * 32 + ax1_0 * 16, 4224, 1, dtype="handle"), 264, "col_major", dtype="handle"))
                                                for ax0_0_3, ax1_0_3, ax2_0_2 in T.grid(2, 2, 2):
                                                    with T.block("C_o_update"):
                                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax1_0_3 * 16 : ax1_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16])
                                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                        T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 2 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax1_0_3 * 2 + ax2_0_2, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 2 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, ax0_0 * 512 + ax1_0 * 16, 512, 2, dtype="handle"), 32, "row_major", dtype="handle"))
                            for ax0 in T.serial(32):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_x * 8 + ax1_3 < 32)
                                        T.reads(C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y // 4 * 64 + blockIdx_x // 16 * 32 + ax0, blockIdx_y % 4 * 512 + blockIdx_x % 16 * 32 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(4, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(64, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(32, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y * 512 + blockIdx_x // 16 * 128 : blockIdx_y * 512 + blockIdx_x // 16 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 512])
                            T.writes(C[blockIdx_y * 512 + blockIdx_x // 16 * 128 : blockIdx_y * 512 + blockIdx_x // 16 * 128 + 128, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 4096])
                            C_reindex_shared = T.alloc_buffer([128, 128], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y * 512 + blockIdx_x // 16 * 128 : blockIdx_y * 512 + blockIdx_x // 16 * 128 + 128, 0 : 512], B[0 : 512, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 512])
                                T.writes(C_reindex_shared[threadIdx_y // 4 * 16 : threadIdx_y // 4 * 16 + 16, threadIdx_y % 4 * 32 : threadIdx_y % 4 * 32 + 32])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([16, 32], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init in T.serial(2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(32):
                                    with T.block():
                                        T.reads(A[blockIdx_y * 512 + blockIdx_x // 16 * 128 : blockIdx_y * 512 + blockIdx_x // 16 * 128 + 128, ax2_0_0 * 16 : ax2_0_0 * 16 + 16], B[ax2_0_0 * 16 : ax2_0_0 * 16 + 16, blockIdx_x % 16 * 128 : blockIdx_x % 16 * 128 + 512], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 32])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 32])
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 16], dtype="float16", scope="wmma.matrix_b")
                                        B_reindex_shared = T.alloc_buffer([128, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([128, 16], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(2):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y * 512 + blockIdx_x // 16 * 128 + ax0_ax1_fused_0 * 64 + threadIdx_y * 2 + threadIdx_x // 16, ax2_0_0 * 16 + threadIdx_x % 16])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 * 64 + threadIdx_y * 2 + threadIdx_x // 16, threadIdx_x % 16])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 * 64 + threadIdx_y * 2 + threadIdx_x // 16, threadIdx_x % 16] = A[blockIdx_y * 512 + blockIdx_x // 16 * 128 + ax0_ax1_fused_0 * 64 + threadIdx_y * 2 + threadIdx_x // 16, ax2_0_0 * 16 + threadIdx_x % 16]
                                        for ax0_ax1_fused_3 in T.vectorized(8):
                                            with T.block("B_reindex_shared"):
                                                T.where(threadIdx_y * 256 + threadIdx_x * 8 + ax0_ax1_fused_3 < 2048)
                                                T.reads(B[ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16, blockIdx_x % 16 * 128 + threadIdx_y * 16 + threadIdx_x // 2])
                                                T.writes(B_reindex_shared[threadIdx_y * 16 + threadIdx_x // 2, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[threadIdx_y * 16 + threadIdx_x // 2, (threadIdx_x * 8 + ax0_ax1_fused_3) % 16] = B[ax2_0_0 * 16 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 16, blockIdx_x % 16 * 128 + threadIdx_y * 16 + threadIdx_x // 2]
                                        with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                            T.reads(A_reindex_shared[threadIdx_y // 4 * 16 : threadIdx_y // 4 * 16 + 16, 0 : 16])
                                            T.writes(A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16])
                                            T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y // 4 * 640, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax0_0 in T.serial(2):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[threadIdx_y % 4 * 32 + ax0_0 * 16 : threadIdx_y % 4 * 32 + ax0_0 * 16 + 16, 0 : 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, 0 : 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, threadIdx_y % 4 * 1280 + ax0_0 * 640, 640, 1, dtype="handle"), 40, "col_major", dtype="handle"))
                                        for ax1_0_3 in T.serial(2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16], B_reindex_shared_wmma_matrix_b[ax1_0_3 * 16 : ax1_0_3 * 16 + 16, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax1_0_3, A_reindex_shared_wmma_matrix_a.data, 0, B_reindex_shared_wmma_matrix_b.data, ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax1_0_3, dtype="handle"))
                                for ax1_0 in T.serial(2):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[0 : 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y // 4 * 16 : threadIdx_y // 4 * 16 + 16, threadIdx_y % 4 * 32 + ax1_0 * 16 : threadIdx_y % 4 * 32 + ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y // 4 * 2048 + threadIdx_y % 4 * 32 + ax1_0 * 16, 2048, 2, dtype="handle"), 128, "row_major", dtype="handle"))
                            for ax0 in T.serial(128):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 128 + threadIdx_x * 4 + ax1_3 < 128)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y * 512 + blockIdx_x // 16 * 128 + ax0, threadIdx_y * 128 + blockIdx_x % 16 * 128 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y * 512 + blockIdx_x // 16 * 128 + ax0, threadIdx_y * 128 + blockIdx_x % 16 * 128 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(32, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(16, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 : blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 + 128])
                            T.writes(C[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + 64, blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 : blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 + 512])
                            C_reindex_shared = T.alloc_buffer([64, 128], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 : blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 + 128])
                                T.writes(C_reindex_shared[threadIdx_y * 32 : threadIdx_y * 32 + 32, 0 : 128])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 128], dtype="float16", scope="wmma.accumulator")
                                for ax1_0_3_init, ax0_0_4_init in T.grid(8, 2):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4_init * 16 : ax0_0_4_init * 16 + 16, ax1_0_3_init * 16 : ax1_0_3_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_4_init * 8 + ax1_0_3_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 : blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + 64, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 : blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 + 128], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 128])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 128])
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([32, 128], dtype="float16", scope="wmma.matrix_b")
                                        A_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 32], dtype="float16", scope="wmma.matrix_a")
                                        B_reindex_shared = T.alloc_buffer([32, 128], dtype="float16", strides=[136, 1], scope="shared")
                                        for ax0_ax1_fused_0 in T.serial(32):
                                            with T.block("A_reindex_shared"):
                                                T.reads(A[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 32 + threadIdx_x])
                                                T.writes(A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x] = A[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + ax0_ax1_fused_0 * 2 + threadIdx_y, ax2_0_0 * 32 + threadIdx_x]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_y, blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 + (threadIdx_x * 4 + ax0_ax1_fused_3)])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x * 4 + ax0_ax1_fused_3])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 2 + threadIdx_y, threadIdx_x * 4 + ax0_ax1_fused_3] = B[ax2_0_0 * 32 + ax0_ax1_fused_0 * 2 + threadIdx_y, blockIdx_y % 2 * 1024 + blockIdx_x % 8 * 128 + (threadIdx_x * 4 + ax0_ax1_fused_3)]
                                        for ax0_0, ax1_0 in T.grid(2, 2):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 2 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 1280 + ax0_0 * 640 + ax1_0 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                        for ax0_0, ax1_0 in T.grid(2, 8):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 8 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 2176 + ax1_0 * 16, 2176, 1, dtype="handle"), 136, "row_major", dtype="handle"))
                                        for ax1_0_3, ax2_0_2, ax0_0_4 in T.grid(8, 2, 2):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_4 * 16 : ax0_0_4 * 16 + 16, ax1_0_3 * 16 : ax1_0_3 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 8 + ax1_0_3, A_reindex_shared_wmma_matrix_a.data, ax0_0_4 * 2 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax2_0_2 * 8 + ax1_0_3, C_reindex_shared_wmma_accumulator.data, ax0_0_4 * 8 + ax1_0_3, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 8):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 8 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 4096 + ax0_0 * 2048 + ax1_0 * 16, 2048, 2, dtype="handle"), 128, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                for ax1_3 in T.vectorized(8):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 256 + threadIdx_x * 8 + ax1_3 < 128)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3])
                                        T.writes(C[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + ax0, blockIdx_y % 2 * 1024 + threadIdx_y * 256 + blockIdx_x % 8 * 128 + threadIdx_x * 8 + ax1_3])
                                        C[blockIdx_y // 2 * 128 + blockIdx_x // 8 * 64 + ax0, blockIdx_y % 2 * 1024 + threadIdx_y * 256 + blockIdx_x % 8 * 128 + threadIdx_x * 8 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 256 + threadIdx_x * 8 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(16, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(32, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(32, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + 128, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 128])
                            T.writes(C[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + 64, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 4096])
                            C_reindex_shared = T.alloc_buffer([64, 128], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + 128, 0 : 512], B[0 : 512, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 128])
                                T.writes(C_reindex_shared[threadIdx_y // 8 * 16 : threadIdx_y // 8 * 16 + 16, threadIdx_y % 8 * 16 : threadIdx_y % 8 * 16 + 16])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.accumulator")
                                with T.block("C_o_init"):
                                    T.reads()
                                    T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                    T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, 0, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(16):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 : blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + 128, ax2_0_0 * 32 : ax2_0_0 * 32 + 32], B[ax2_0_0 * 32 : ax2_0_0 * 32 + 32, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 : blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + 128], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16])
                                        A_reindex_shared = T.alloc_buffer([64, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        B_reindex_shared = T.alloc_buffer([128, 32], dtype="float16", strides=[40, 1], scope="shared")
                                        for ax0_ax1_fused_3 in T.vectorized(4):
                                            with T.block("A_reindex_shared"):
                                                T.where(threadIdx_y * 128 + threadIdx_x * 4 + ax0_ax1_fused_3 < 2048)
                                                T.reads(A[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + threadIdx_y * 4 + threadIdx_x // 8, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                T.writes(A_reindex_shared[threadIdx_y * 4 + threadIdx_x // 8, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                A_reindex_shared[threadIdx_y * 4 + threadIdx_x // 8, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32] = A[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + threadIdx_y * 4 + threadIdx_x // 8, ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32]
                                        for ax0_ax1_fused_3 in T.vectorized(4):
                                            with T.block("B_reindex_shared"):
                                                T.reads(B[ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + threadIdx_y * 4 + threadIdx_x // 8])
                                                T.writes(B_reindex_shared[threadIdx_y * 4 + threadIdx_x // 8, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32])
                                                T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                B_reindex_shared[threadIdx_y * 4 + threadIdx_x // 8, (threadIdx_x * 4 + ax0_ax1_fused_3) % 32] = B[ax2_0_0 * 32 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 32, blockIdx_y % 4 * 512 + blockIdx_x % 4 * 128 + threadIdx_y * 4 + threadIdx_x // 8]
                                        for ax2_0_1 in T.serial(2):
                                            with T.block():
                                                T.reads(A_reindex_shared[threadIdx_y // 8 * 16 : threadIdx_y // 8 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], B_reindex_shared[threadIdx_y % 8 * 16 : threadIdx_y % 8 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16], C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16])
                                                A_reindex_shared_wmma_matrix_a = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_a")
                                                B_reindex_shared_wmma_matrix_b = T.alloc_buffer([16, 16], dtype="float16", scope="wmma.matrix_b")
                                                with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                    T.reads(A_reindex_shared[threadIdx_y // 8 * 16 : threadIdx_y // 8 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                    T.writes(A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16])
                                                    T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y // 8 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "row_major", dtype="handle"))
                                                with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                    T.reads(B_reindex_shared[threadIdx_y % 8 * 16 : threadIdx_y % 8 * 16 + 16, ax2_0_1 * 16 : ax2_0_1 * 16 + 16])
                                                    T.writes(B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                    T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, threadIdx_y % 8 * 640 + ax2_0_1 * 16, 640, 1, dtype="handle"), 40, "col_major", dtype="handle"))
                                                with T.block("C_o_update"):
                                                    T.reads(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16], A_reindex_shared_wmma_matrix_a[0 : 16, 0 : 16], B_reindex_shared_wmma_matrix_b[0 : 16, 0 : 16])
                                                    T.writes(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16])
                                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                    T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, 0, A_reindex_shared_wmma_matrix_a.data, 0, B_reindex_shared_wmma_matrix_b.data, 0, C_reindex_shared_wmma_accumulator.data, 0, dtype="handle"))
                                with T.block("C_reindex_shared_wmma.accumulator_o"):
                                    T.reads(C_reindex_shared_wmma_accumulator[0 : 16, 0 : 16])
                                    T.writes(C_reindex_shared[threadIdx_y // 8 * 16 : threadIdx_y // 8 * 16 + 16, threadIdx_y % 8 * 16 : threadIdx_y % 8 * 16 + 16])
                                    T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, 0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y // 8 * 2048 + threadIdx_y % 8 * 16, 2048, 2, dtype="handle"), 128, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                for ax1_3 in T.vectorized(4):
                                    with T.block("C_reindex_shared"):
                                        T.where(threadIdx_y * 128 + threadIdx_x * 4 + ax1_3 < 128)
                                        T.reads(C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3])
                                        T.writes(C[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + ax0, blockIdx_y % 4 * 512 + threadIdx_y * 128 + blockIdx_x % 4 * 128 + threadIdx_x * 4 + ax1_3])
                                        C[blockIdx_y // 4 * 512 + blockIdx_x // 4 * 64 + ax0, blockIdx_y % 4 * 512 + threadIdx_y * 128 + blockIdx_x % 4 * 128 + threadIdx_x * 4 + ax1_3] = C_reindex_shared[ax0, threadIdx_y * 128 + threadIdx_x * 4 + ax1_3]
    
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(2048, 512), "float16"], B: T.Buffer[(512, 2048), "float16"], C: T.Buffer[(2048, 2048), "float16"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for blockIdx_y in T.thread_binding(256, thread="blockIdx.y", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for blockIdx_x in T.thread_binding(4, thread="blockIdx.x"):
                for threadIdx_y in T.thread_binding(2, thread="threadIdx.y"):
                    for threadIdx_x in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block():
                            T.reads(A[blockIdx_y // 8 * 64 : blockIdx_y // 8 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 8 * 256 + blockIdx_x * 64 : blockIdx_y % 8 * 256 + blockIdx_x * 64 + 64])
                            T.writes(C[blockIdx_y // 8 * 64 : blockIdx_y // 8 * 64 + 64, blockIdx_y % 8 * 256 + blockIdx_x * 64 : blockIdx_y % 8 * 256 + blockIdx_x * 64 + 64])
                            C_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", scope="shared")
                            with T.block():
                                T.reads(A[blockIdx_y // 8 * 64 : blockIdx_y // 8 * 64 + 64, 0 : 512], B[0 : 512, blockIdx_y % 8 * 256 + blockIdx_x * 64 : blockIdx_y % 8 * 256 + blockIdx_x * 64 + 64])
                                T.writes(C_reindex_shared[threadIdx_y * 32 : threadIdx_y * 32 + 32, 0 : 64])
                                C_reindex_shared_wmma_accumulator = T.alloc_buffer([32, 64], dtype="float16", scope="wmma.accumulator")
                                for ax0_0_3_init, ax1_0_4_init in T.grid(2, 4):
                                    with T.block("C_o_init"):
                                        T.reads()
                                        T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3_init * 16 : ax0_0_3_init * 16 + 16, ax1_0_4_init * 16 : ax1_0_4_init * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        T.evaluate(T.tvm_fill_fragment(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0_3_init * 4 + ax1_0_4_init, T.float32(0), dtype="handle"))
                                for ax2_0_0 in T.serial(8):
                                    with T.block():
                                        T.reads(A[blockIdx_y // 8 * 64 : blockIdx_y // 8 * 64 + 64, ax2_0_0 * 64 : ax2_0_0 * 64 + 64], B[ax2_0_0 * 64 : ax2_0_0 * 64 + 64, blockIdx_y % 8 * 256 + blockIdx_x * 64 : blockIdx_y % 8 * 256 + blockIdx_x * 64 + 64], C_reindex_shared_wmma_accumulator[0 : 32, 0 : 64])
                                        T.writes(C_reindex_shared_wmma_accumulator[0 : 32, 0 : 64])
                                        B_reindex_shared_wmma_matrix_b = T.alloc_buffer([64, 64], dtype="float16", scope="wmma.matrix_b")
                                        B_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared = T.alloc_buffer([64, 64], dtype="float16", strides=[72, 1], scope="shared")
                                        A_reindex_shared_wmma_matrix_a = T.alloc_buffer([32, 64], dtype="float16", scope="wmma.matrix_a")
                                        for ax0_ax1_fused_0 in T.serial(8):
                                            for ax0_ax1_fused_3 in T.vectorized(8):
                                                with T.block("A_reindex_shared"):
                                                    T.reads(A[blockIdx_y // 8 * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, ax2_0_0 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.writes(A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    A_reindex_shared[ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, (threadIdx_x * 8 + ax0_ax1_fused_3) % 64] = A[blockIdx_y // 8 * 64 + ax0_ax1_fused_0 * 8 + threadIdx_y * 4 + (threadIdx_x * 8 + ax0_ax1_fused_3) // 64, ax2_0_0 * 64 + (threadIdx_x * 8 + ax0_ax1_fused_3) % 64]
                                        for ax0_ax1_fused_0 in T.serial(16):
                                            for ax0_ax1_fused_3 in T.vectorized(4):
                                                with T.block("B_reindex_shared"):
                                                    T.reads(B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, blockIdx_y % 8 * 256 + blockIdx_x * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.writes(B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64])
                                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]]})
                                                    B_reindex_shared[ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, (threadIdx_x * 4 + ax0_ax1_fused_3) % 64] = B[ax2_0_0 * 64 + ax0_ax1_fused_0 * 4 + threadIdx_y * 2 + (threadIdx_x * 4 + ax0_ax1_fused_3) // 64, blockIdx_y % 8 * 256 + blockIdx_x * 64 + (threadIdx_x * 4 + ax0_ax1_fused_3) % 64]
                                        for ax0_0, ax1_0 in T.grid(2, 4):
                                            with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                                T.reads(A_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(A_reindex_shared_wmma_matrix_a[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(A_reindex_shared_wmma_matrix_a.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), A_reindex_shared.data, threadIdx_y * 2304 + ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0, ax1_0 in T.grid(4, 4):
                                            with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                                T.reads(B_reindex_shared[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.writes(B_reindex_shared_wmma_matrix_b[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                                T.evaluate(T.tvm_load_matrix_sync(B_reindex_shared_wmma_matrix_b.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), B_reindex_shared.data, ax0_0 * 1152 + ax1_0 * 16, 1152, 1, dtype="handle"), 72, "row_major", dtype="handle"))
                                        for ax0_0_3, ax2_0_2, ax1_0_4 in T.grid(2, 4, 4):
                                            with T.block("C_o_update"):
                                                T.reads(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16], A_reindex_shared_wmma_matrix_a[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax2_0_2 * 16 : ax2_0_2 * 16 + 16], B_reindex_shared_wmma_matrix_b[ax2_0_2 * 16 : ax2_0_2 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16])
                                                T.writes(C_reindex_shared_wmma_accumulator[ax0_0_3 * 16 : ax0_0_3 * 16 + 16, ax1_0_4 * 16 : ax1_0_4 * 16 + 16])
                                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                                T.evaluate(T.tvm_mma_sync(C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax2022-11-27 18:53:10 [DEBUG] XGB iter   0: tr-p-rmse: 0.577452	tr-a-peak@32: 0.876896	tr-rmse: 0.336799	tr-rmse: 0.336799
2022-11-27 18:53:10 [DEBUG] XGB iter  25: tr-p-rmse: 0.040854	tr-a-peak@32: 1.000000	tr-rmse: 0.409666	tr-rmse: 0.409666
2022-11-27 18:53:10 [DEBUG] XGB iter  50: tr-p-rmse: 0.040854	tr-a-peak@32: 1.000000	tr-rmse: 0.409666	tr-rmse: 0.409666
2022-11-27 18:53:10 [DEBUG] XGB stopped. Best iteration: [18] tr-p-rmse:0.04085	tr-a-peak@32:1.00000	tr-rmse:0.40967	tr-rmse:0.40967 
1_0_4, A_reindex_shared_wmma_matrix_a.data, ax0_0_3 * 4 + ax2_0_2, B_reindex_shared_wmma_matrix_b.data, ax2_0_2 * 4 + ax1_0_4, C_reindex_shared_wmma_accumulator.data, ax0_0_3 * 4 + ax1_0_4, dtype="handle"))
                                for ax0_0, ax1_0 in T.grid(2, 4):
                                    with T.block("C_reindex_shared_wmma.accumulator_o"):
                                        T.reads(C_reindex_shared_wmma_accumulator[ax0_0 * 16 : ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.writes(C_reindex_shared[threadIdx_y * 32 + ax0_0 * 16 : threadIdx_y * 32 + ax0_0 * 16 + 16, ax1_0 * 16 : ax1_0 * 16 + 16])
                                        T.evaluate(T.tvm_store_matrix_sync(C_reindex_shared_wmma_accumulator.data, 16, 16, 16, ax0_0 * 4 + ax1_0, T.tvm_access_ptr(T.type_annotation(dtype="float16"), C_reindex_shared.data, threadIdx_y * 2048 + ax0_0 * 1024 + ax1_0 * 16, 1024, 2, dtype="handle"), 64, "row_major", dtype="handle"))
                            for ax0 in T.serial(64):
                                with T.block("C_reindex_shared"):
                                    T.reads(C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x])
                                    T.writes(C[blockIdx_y // 8 * 64 + ax0, blockIdx_y % 8 * 256 + blockIdx_x * 64 + threadIdx_y * 32 + threadIdx_x])
                                    C[blockIdx_y // 8 * 64 + ax0, blockIdx_y % 8 * 256 + blockIdx_x * 64 + threadIdx_y * 32 + threadIdx_x] = C_reindex_shared[ax0, threadIdx_y * 32 + threadIdx_x]
    
2022-11-27 18:53:10 [INFO] [task_scheduler.cc:235] [Updated] Task #0: "main"
2022-11-27 18:53:10 [INFO] [task_scheduler.cc:318] 
 ID | Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done 
---------------------------------------------------------------------------------------------------------
  0 | main | 4294967296 |      1 |     69867.1252 |      61.4734 |               61.4734 |     32 |      
---------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 61.4734

2022-11-27 18:53:10 [INFO] [task_scheduler.cc:258] Task #0 has finished. Remaining task(s): 0
2022-11-27 18:53:10 [INFO] [task_scheduler.cc:318] 
 ID | Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done 
---------------------------------------------------------------------------------------------------------
  0 | main | 4294967296 |      1 |     69867.1252 |      61.4734 |               61.4734 |     32 |    Y 
---------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 61.4734

