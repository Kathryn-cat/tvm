# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(256, 256), "float16"], B: T.Buffer[(256, 256), "float16"], C: T.Buffer[(256, 256), "float16"]) -> None:
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            C_reindex_shared = T.alloc_buffer([256, 256], dtype="float16", scope="shared")
            C_reindex_shared_wmma_accumulator = T.alloc_buffer([256, 256], dtype="float16", scope="wmma.accumulator")
            A_reindex_shared = T.alloc_buffer([256, 256], dtype="float16", scope="shared")
            B_reindex_shared = T.alloc_buffer([256, 256], dtype="float16", scope="shared")
            A_reindex_shared_wmma_matrix_a = T.alloc_buffer([256, 256], dtype="float16", scope="wmma.matrix_a")
            B_reindex_shared_wmma_matrix_b = T.alloc_buffer([256, 256], dtype="float16", scope="wmma.matrix_b")
            for ax0_0_0_ax1_0_0_fused in T.thread_binding(8, thread="blockIdx.y"):
                for ax0_0_1_ax1_0_1_fused in T.thread_binding(4, thread="blockIdx.x"):
                    for ax0_0_2_ax1_0_2_fused in T.thread_binding(1, thread="threadIdx.y"):
                        for ax2_0_0 in T.serial(1):
                            for ax0_ax1_fused in T.serial(8192):
                                with T.block("A_reindex_shared"):
                                    v0 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused // 2 * 64 + ax0_0_1_ax1_0_1_fused // 2 * 32 + ax0_ax1_fused // 256)
                                    v1 = T.axis.spatial(256, ax0_ax1_fused % 256)
                                    T.reads(A[v0, v1])
                                    T.writes(A_reindex_shared[v0, v1])
                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]], "meta_schedule.cooperative_fetch":2})
                                    A_reindex_shared[v0, v1] = A[v0, v1]
                            for ax0_ax1_fused in T.serial(16384):
                                with T.block("B_reindex_shared"):
                                    v0 = T.axis.spatial(256, ax0_ax1_fused // 64)
                                    v1 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused % 2 * 128 + ax0_0_1_ax1_0_1_fused % 2 * 64 + ax0_ax1_fused % 64)
                                    T.reads(B[v0, v1])
                                    T.writes(B_reindex_shared[v0, v1])
                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]], "meta_schedule.cooperative_fetch":8})
                                    B_reindex_shared[v0, v1] = B[v0, v1]
                            for ax2_0_1 in T.serial(4):
                                for ax0_0, ax1_0 in T.grid(2, 4):
                                    with T.block("A_reindex_shared_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(16, ax0_0_0_ax1_0_0_fused // 2 * 4 + ax0_0_1_ax1_0_1_fused // 2 * 2 + ax0_0)
                                        v1_o = T.axis.spatial(16, ax2_0_1 * 4 + ax1_0)
                                        T.reads(A_reindex_shared[v0_o * 16 : v0_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                        T.writes(A_reindex_shared_wmma_matrix_a[v0_o * 16 : v0_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_load_16x16x16_f16_a"})
                                        for ax0_1, ax1_1 in T.grid(16, 16):
                                            with T.block("A_reindex_shared_wmma.matrix_a"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                                T.reads(A_reindex_shared[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                T.writes(A_reindex_shared_wmma_matrix_a[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                A_reindex_shared_wmma_matrix_a[v0_o * 16 + v0_i, v1_o * 16 + v1_i] = A_reindex_shared[v0_o * 16 + v0_i, v1_o * 16 + v1_i]
                                for ax0_0, ax1_0 in T.grid(4, 4):
                                    with T.block("B_reindex_shared_wmma.matrix_b_o"):
                                        v0_o = T.axis.spatial(16, ax2_0_1 * 4 + ax0_0)
                                        v1_o = T.axis.spatial(16, ax0_0_0_ax1_0_0_fused % 2 * 8 + ax0_0_1_ax1_0_1_fused % 2 * 4 + ax1_0)
                                        T.reads(B_reindex_shared[v0_o * 16 : v0_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                        T.writes(B_reindex_shared_wmma_matrix_b[v0_o * 16 : v0_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_load_16x16x16_f16_b"})
                                        for ax0_1, ax1_1 in T.grid(16, 16):
                                            with T.block("B_reindex_shared_wmma.matrix_b"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                                T.reads(B_reindex_shared[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                T.writes(B_reindex_shared_wmma_matrix_b[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                B_reindex_shared_wmma_matrix_b[v0_o * 16 + v0_i, v1_o * 16 + v1_i] = B_reindex_shared[v0_o * 16 + v0_i, v1_o * 16 + v1_i]
                                for ax0_0_3, ax1_0_3, ax2_0_2, ax0_0_4, ax1_0_4 in T.grid(2, 2, 4, 1, 2):
                                    with T.block("update_o"):
                                        v0_o = T.axis.spatial(16, ax0_0_0_ax1_0_0_fused // 2 * 4 + ax0_0_1_ax1_0_1_fused // 2 * 2 + ax0_0_3 + ax0_0_4)
                                        v1_o = T.axis.spatial(16, ax0_0_0_ax1_0_0_fused % 2 * 8 + ax0_0_1_ax1_0_1_fused % 2 * 4 + ax1_0_3 * 2 + ax1_0_4)
                                        v2_o = T.axis.reduce(16, ax2_0_0 * 16 + ax2_0_1 * 4 + ax2_0_2)
                                        T.reads(A_reindex_shared_wmma_matrix_a[v0_o * 16 : v0_o * 16 + 16, v2_o * 16 : v2_o * 16 + 16], B_reindex_shared_wmma_matrix_b[v2_o * 16 : v2_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                        T.writes(C_reindex_shared_wmma_accumulator[v0_o * 16 : v0_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_sync_16x16x16_f16f16f16", "meta_schedule.auto_tensorize_init":"wmma_fill_16x16x16_f16", "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        with T.init():
                                            for ax0_1, ax1_1 in T.grid(16, 16):
                                                with T.block("update_init"):
                                                    v0_i_init, v1_i_init = T.axis.remap("SS", [ax0_1, ax1_1])
                                                    T.reads()
                                                    T.writes(C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i_init, v1_o * 16 + v1_i_init])
                                                    C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i_init, v1_o * 16 + v1_i_init] = T.float16(0)
                                        for ax0_1, ax1_1, ax2_1 in T.grid(16, 16, 16):
                                            with T.block("update"):
                                                v0_i, v1_i, v2_i = T.axis.remap("SSR", [ax0_1, ax1_1, ax2_1])
                                                T.reads(C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i, v1_o * 16 + v1_i], A_reindex_shared_wmma_matrix_a[v0_o * 16 + v0_i, v2_o * 16 + v2_i], B_reindex_shared_wmma_matrix_b[v2_o * 16 + v2_i, v1_o * 16 + v1_i])
                                                T.writes(C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                                T.block_attr({"meta_schedule.tiling_structure":"SSSRRSRS"})
                                                C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i, v1_o * 16 + v1_i] = C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i, v1_o * 16 + v1_i] + A_reindex_shared_wmma_matrix_a[v0_o * 16 + v0_i, v2_o * 16 + v2_i] * B_reindex_shared_wmma_matrix_b[v2_o * 16 + v2_i, v1_o * 16 + v1_i]
                        for ax0_0, ax1_0 in T.grid(2, 4):
                            with T.block("C_reindex_shared_wmma.accumulator_o"):
                                v0_o = T.axis.spatial(16, ax0_0_0_ax1_0_0_fused // 2 * 4 + ax0_0_1_ax1_0_1_fused // 2 * 2 + ax0_0)
                                v1_o = T.axis.spatial(16, ax0_0_0_ax1_0_0_fused % 2 * 8 + ax0_0_1_ax1_0_1_fused % 2 * 4 + ax1_0)
                                T.reads(C_reindex_shared_wmma_accumulator[v0_o * 16 : v0_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                T.writes(C_reindex_shared[v0_o * 16 : v0_o * 16 + 16, v1_o * 16 : v1_o * 16 + 16])
                                T.block_attr({"meta_schedule.auto_tensorize":"wmma_store_16x16x16_f16_shared"})
                                for ax0_1, ax1_1 in T.grid(16, 16):
                                    with T.block("C_reindex_shared_wmma.accumulator"):
                                        v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                        T.reads(C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                        T.writes(C_reindex_shared[v0_o * 16 + v0_i, v1_o * 16 + v1_i])
                                        C_reindex_shared[v0_o * 16 + v0_i, v1_o * 16 + v1_i] = C_reindex_shared_wmma_accumulator[v0_o * 16 + v0_i, v1_o * 16 + v1_i]
                    for ax0, ax1 in T.grid(32, 64):
                        with T.block("C_reindex_shared"):
                            v0 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused // 2 * 64 + ax0_0_1_ax1_0_1_fused // 2 * 32 + ax0)
                            v1 = T.axis.spatial(256, ax0_0_0_ax1_0_0_fused % 2 * 128 + ax0_0_1_ax1_0_1_fused % 2 * 64 + ax1)
                            T.reads(C_reindex_shared[v0, v1])
                            T.writes(C[v0, v1])
                            T.block_attr({"meta_schedule.cooperative_fetch":8})
                            C[v0, v1] = C_reindex_shared[v0, v1]
